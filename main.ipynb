{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "import utils\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# custom modules\n",
    "from data_acquisition import DataHandler\n",
    "from data_preparation import apply_preprocessing_pipeline\n",
    "\n",
    "\n",
    "# Configure logging for the pipeline\n",
    "logger = utils.setup_logger(level='ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['London', 'CapeTown', 'Hamburg', 'Johannesburg', 'London', 'Montreal', 'Paris', 'Seoul', 'Singapore', 'Sydney']\n",
    "\n",
    "datahandler = DataHandler(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf242d1c784248e0a095f8f6422dffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
     ]
    }
   ],
   "source": [
    "# load images and mask for all specified cites\n",
    "\n",
    "import os\n",
    "images = []\n",
    "sparse_masks=[]\n",
    "dense_masks=[]\n",
    "\n",
    "for city in tqdm(cities):\n",
    "    buildings = None\n",
    "    if not os.path.exists(f'data/{city}/building_mask_dense.tif'):\n",
    "        print(\"loading local buildings\")\n",
    "        buildings = datahandler.get_buildings(city)\n",
    "    images.append(datahandler.get_satellite_image(city))\n",
    "    sparse_masks.append(datahandler.get_building_mask(city, all_touched=False, loaded_buildings=buildings))\n",
    "    dense_masks.append(datahandler.get_building_mask(city, all_touched=True, loaded_buildings=buildings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = sparse_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Differences of train and test set is below 10% on mean, std, min and max across all input bands\n",
      "✓ Differences of train and validation set is below 10% on mean, std, min and max across all input bands\n",
      "\n",
      "Comparison of distribution of masks:\n",
      "                   mean  median          std  10th percentile  90th percentile\n",
      "Train       600.190379   116.0  1032.131645              0.0           2023.6\n",
      "Test        593.828725   114.5  1028.243032              0.0           2054.9\n",
      "Validation  592.890937   103.5  1025.142817              0.0           2072.5\n",
      "\n",
      "Comparison of cities the data in the differen sets originates from:\n",
      "               London  CapeTown   Hamburg  Johannesburg\n",
      "Train       0.334186  0.142272  0.143296      0.380246\n",
      "Test        0.319508  0.150538  0.146697      0.383257\n",
      "Validation  0.311828  0.159754  0.137481      0.390937\n"
     ]
    }
   ],
   "source": [
    "# apply training pipeline\n",
    "# TODO make train test split consistent so we can train with multiple sizes, dont know if there is an advantage though\n",
    "train_loader, test_loader , validation_loader= apply_preprocessing_pipeline(images, masks, patch_size = 128, test_ratio= 0.2,validation_ratio=0.2, batch_size = 64, show_validation_of_split=True,city_names=cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 7, 128, 128]), torch.Tensor, torch.uint16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_loader))   \n",
    "sample.shape, type(sample), sample.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Conv2d(6, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "                nn.Conv2d(128, 1, kernel_size=1, padding=0),\n",
    "                nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class LitNet(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[:,:-1], batch[:,-1]\n",
    "        outs = self.model(x.float())\n",
    "        loss = self.loss(outs, y.unsqueeze(1).float())\n",
    "        self.log(\"train_loss\", value=loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch[:,:-1], batch[:,-1]\n",
    "        outs = self.model(x.float())\n",
    "        loss = self.loss(outs, y.unsqueeze(1).float())\n",
    "        \n",
    "        values = {\n",
    "            \"test_loss\": loss,\n",
    "        }\n",
    "        self.log_dict(values, on_epoch=True, on_step=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model\n",
    "convmodel = LitNet(convNetSimple())\n",
    "model_dict = torch.load(\"models/lightning_logs/version_1/checkpoints/epoch=39-step=7000.ckpt\", map_location=torch.device('cpu'))\n",
    "convmodel.load_state_dict(model_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n",
      "2024-06-28 23:29:49,796 - lightning.fabric.utilities.seed - INFO - seed_everything - Seed set to 42\n",
      "2024-06-28 23:29:49,796 - lightning.fabric.utilities.seed - INFO - seed_everything - Seed set to 42\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "2024-06-28 23:29:49,816 - lightning.pytorch.utilities.rank_zero - INFO - _info - GPU available: True (cuda), used: True\n",
      "2024-06-28 23:29:49,816 - lightning.pytorch.utilities.rank_zero - INFO - _info - GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "2024-06-28 23:29:49,818 - lightning.pytorch.utilities.rank_zero - INFO - _info - TPU available: False, using: 0 TPU cores\n",
      "2024-06-28 23:29:49,818 - lightning.pytorch.utilities.rank_zero - INFO - _info - TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "2024-06-28 23:29:49,819 - lightning.pytorch.utilities.rank_zero - INFO - _info - HPU available: False, using: 0 HPUs\n",
      "2024-06-28 23:29:49,819 - lightning.pytorch.utilities.rank_zero - INFO - _info - HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 23:29:49,823 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 23:29:49,823 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | convNetSimple | 94.2 K | train\n",
      "1 | loss  | BCELoss       | 0      | train\n",
      "------------------------------------------------\n",
      "94.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.2 K    Total params\n",
      "0.377     Total estimated model params size (MB)\n",
      "2024-06-28 23:29:49,836 - lightning.pytorch.callbacks.model_summary - INFO - summarize - \n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | convNetSimple | 94.2 K | train\n",
      "1 | loss  | BCELoss       | 0      | train\n",
      "------------------------------------------------\n",
      "94.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.2 K    Total params\n",
      "0.377     Total estimated model params size (MB)\n",
      "2024-06-28 23:29:49,836 - lightning.pytorch.callbacks.model_summary - INFO - summarize - \n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | convNetSimple | 94.2 K | train\n",
      "1 | loss  | BCELoss       | 0      | train\n",
      "------------------------------------------------\n",
      "94.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.2 K    Total params\n",
      "0.377     Total estimated model params size (MB)\n",
      "/home/jlb/Dev/architecture-of-ml-systems/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=20` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a2bdef495f4e469644770c2113461d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "2024-06-28 23:47:55,487 - lightning.pytorch.utilities.rank_zero - INFO - _info - `Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "2024-06-28 23:47:55,487 - lightning.pytorch.utilities.rank_zero - INFO - _info - `Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 23:47:55,519 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 23:47:55,519 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jlb/Dev/architecture-of-ml-systems/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=20` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b6783770584ec982a5b3b9755c6e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_loss_epoch         9.542298316955566\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 9.542298316955566}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "\n",
    "L.seed_everything(42)\n",
    "convmodel = LitNet(convNetSimple())\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=\"models\",\n",
    "    # callbacks=[\n",
    "    #     EarlyStopping(\n",
    "    #         monitor=\"val_loss\",\n",
    "    #         mode=\"min\",\n",
    "    #         patience=10,\n",
    "    #     )\n",
    "    #     ModelCheckpoint(\n",
    "    #         monitor=\"val_loss\",\n",
    "    #         mode=\"min\",\n",
    "    #         save_top_k=2,\n",
    "    #         dirpath=\"models\",\n",
    "    #         filename=\"best_model\"\n",
    "    #     )\n",
    "    # ]\n",
    "    # val_check_interval=1,\n",
    "    fast_dev_run=False,\n",
    "    # num_sanity_val_steps=2,\n",
    "    max_epochs=40,\n",
    "    log_every_n_steps=20,\n",
    ")\n",
    "\n",
    "# training\n",
    "trainer.fit(convmodel, \n",
    "    train_dataloaders=train_loader,\n",
    "    # val_dataloaders=val_loader   #### HIER brauchen wir noch einen validation loader\n",
    ")\n",
    "\n",
    "\n",
    "# testing\n",
    "\n",
    "# hier könnte man noch das beste model laden, wenn wir ein Val dataset haben.\n",
    "# best_model = LitModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "# trainer.test(\n",
    "#     best_model,\n",
    "#     dataloaders=test_loader\n",
    "# )\n",
    "\n",
    "\n",
    "trainer.test(\n",
    "    convmodel,\n",
    "    dataloaders=test_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model, loss function, and optimizer\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 50\n",
    "\n",
    "# model.train()\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "#     for batch in train_loader:\n",
    "#         # splid in inputs and labels\n",
    "#         inputs = batch[:,:-1].to(torch.float32)\n",
    "#         labels = batch[:,-1, np.newaxis].to(torch.float32)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward pass\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # calculate loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # write to tensorboard\n",
    "#         writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "#         # backward pass\n",
    "#         loss.backward()\n",
    "\n",
    "#         # optimizer step\n",
    "#         optimizer.step()\n",
    "    \n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.makedirs(\"saved_models\", exist_ok=True)\n",
    "# torch.save(model.state_dict(), \"saved_models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9638)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on test set\n",
    "t  = torch.Tensor(test_loader.dataset)\n",
    "\n",
    "# splid in inputs and labels\n",
    "test_inputs = t[:,:-1]#.to(torch.float32)\n",
    "test_labels = t[:,-1, np.newaxis]#.to(torch.float32)\n",
    "\n",
    "test_results = convmodel(test_inputs).detach()\n",
    "\n",
    "# Look at sums, to check if model only predicts zeros\n",
    "print(\"Sum of test results: \", test_results.sum())\n",
    "print(\"But it should be closer to: \", test_labels.sum())\n",
    "\n",
    "\n",
    "# # see how many percnet where predicted right\n",
    "threshold = 0.5\n",
    "((test_results>threshold)==test_labels).sum()/np.prod(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# RocCurveDisplay.from_predictions(\n",
    "#    test_labels.flatten(), test_results.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t  = torch.Tensor(test_loader.dataset)\n",
    "\n",
    "# # splid in inputs and labels\n",
    "# test_inputs = t[:,:-1]#.to(torch.float32)\n",
    "# test_labels = t[:,-1, np.newaxis]#.to(torch.float32)\n",
    "\n",
    "# test_results = model(test_inputs).detach()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "buildings = []\n",
    "sat_images = []\n",
    "building_masks = []\n",
    "\n",
    "for city in cities: \n",
    "    buildings.append(datahandler.get_buildings(city))\n",
    "    sat_images.append(datahandler.get_satellite_image(city))\n",
    "    building_masks.append(datahandler.get_building_mask(city))\n",
    "\n",
    "# Plot the expected results for the first city \n",
    "datahandler.plot(city[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preparation\n",
    "\n",
    "for city in cities:\n",
    "    data_preparation.create_tensor(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download \n",
    "\n",
    "for city in cities: \n",
    "    sat_image = datahandler.get_satellite_image(city)\n",
    "    mask = datahandler.get_building_mask(city)\n",
    "\n",
    "# Plot the expected results for the first city \n",
    "datahandler.plot(city[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
