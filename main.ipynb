{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import utils\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "# custom modules\n",
    "from data_acquisition import DataHandler\n",
    "from data_preparation import apply_preprocessing_pipeline\n",
    "\n",
    "\n",
    "# Configure logging for the pipeline\n",
    "logger = utils.setup_logger(level='ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['London']#, 'CapeTown', 'Hamburg', 'Johannesburg', 'London', 'Montreal', 'Paris', 'Seoul', 'Singapore', 'Sydney']\n",
    "\n",
    "datahandler = DataHandler(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b62d53c1bb34355affe0a0635851af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
     ]
    }
   ],
   "source": [
    "# load images and mask for all specified cites\n",
    "\n",
    "images = []\n",
    "masks=[]\n",
    "for city in tqdm(cities):\n",
    "    images.append(datahandler.get_satellite_image(city))\n",
    "    masks.append(datahandler.get_building_mask(city, all_touched=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply training pipeline\n",
    "# TODO make train test split consistent so we can train with multiple sizes, dont know if there is an advantage though\n",
    "train_loader, test_loader = apply_preprocessing_pipeline(images, masks, patch_size = 128, train_ratio = 0.8, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model, taken from exercise pdf\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(6, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(128, 1, kernel_size=1, padding=0),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "# initialize tensorboard writer\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 7, 128, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_loader))   \n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.uint16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample), sample.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Conv2d(6, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "                nn.Conv2d(128, 1, kernel_size=1, padding=0),\n",
    "                nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitNet(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[:,:-1], batch[:,-1]\n",
    "        outs = self.model(x.float())\n",
    "        loss = self.loss(outs, y.unsqueeze(1).float())\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch[:,:-1], batch[:,-1]\n",
    "        outs = self.model(x.float())\n",
    "        loss = self.loss(outs, y.unsqueeze(1).float())\n",
    "        \n",
    "        values = {\n",
    "            \"test_loss\": loss,\n",
    "        \n",
    "        }\n",
    "        self.log_dict(values, on_epoch=True, on_step=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n",
      "2024-06-28 22:59:42,313 - lightning.fabric.utilities.seed - INFO - seed_everything - Seed set to 42\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "2024-06-28 22:59:42,339 - lightning.pytorch.utilities.rank_zero - INFO - _info - GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "2024-06-28 22:59:42,341 - lightning.pytorch.utilities.rank_zero - INFO - _info - TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "2024-06-28 22:59:42,342 - lightning.pytorch.utilities.rank_zero - INFO - _info - HPU available: False, using: 0 HPUs\n",
      "WARNING: Missing logger folder: models/lightning_logs\n",
      "2024-06-28 22:59:42,344 - lightning.pytorch.loggers.tensorboard - WARNING - _get_next_version - Missing logger folder: models/lightning_logs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 22:59:42,347 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | convNetSimple | 94.2 K | train\n",
      "1 | loss  | BCELoss       | 0      | train\n",
      "------------------------------------------------\n",
      "94.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.2 K    Total params\n",
      "0.377     Total estimated model params size (MB)\n",
      "2024-06-28 22:59:42,372 - lightning.pytorch.callbacks.model_summary - INFO - summarize - \n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | convNetSimple | 94.2 K | train\n",
      "1 | loss  | BCELoss       | 0      | train\n",
      "------------------------------------------------\n",
      "94.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.2 K    Total params\n",
      "0.377     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16b424190eb4db2a3f98abe677b4eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "2024-06-28 23:02:21,949 - lightning.pytorch.utilities.rank_zero - INFO - _info - `Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 23:02:21,978 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6a04b6f14949b59ba3f84a9e80b409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_loss_epoch        5.8461079597473145\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 5.8461079597473145}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "\n",
    "L.seed_everything(42)\n",
    "convmodel = LitNet(convNetSimple())\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=\"models\",\n",
    "    # callbacks=[\n",
    "    #     EarlyStopping(\n",
    "    #         monitor=\"val_loss\",\n",
    "    #         mode=\"min\",\n",
    "    #         patience=10,\n",
    "    #     )\n",
    "    #     ModelCheckpoint(\n",
    "    #         monitor=\"val_loss\",\n",
    "    #         mode=\"min\",\n",
    "    #         save_top_k=2,\n",
    "    #         dirpath=\"models\",\n",
    "    #         filename=\"best_model\"\n",
    "    #     )\n",
    "    # ]\n",
    "    # val_check_interval=1,\n",
    "    fast_dev_run=False,\n",
    "    # num_sanity_val_steps=2,\n",
    "    max_epochs=40,\n",
    "    log_every_n_steps=20,\n",
    ")\n",
    "\n",
    "# training\n",
    "trainer.fit(convmodel, \n",
    "    train_dataloaders=train_loader,\n",
    "    # val_dataloaders=val_loader   #### HIER brauchen wir noch einen validation loader\n",
    ")\n",
    "\n",
    "# testing\n",
    "trainer.test(\n",
    "    convmodel,\n",
    "    dataloaders=test_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model, loss function, and optimizer\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 50\n",
    "\n",
    "# model.train()\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "#     for batch in train_loader:\n",
    "#         # splid in inputs and labels\n",
    "#         inputs = batch[:,:-1].to(torch.float32)\n",
    "#         labels = batch[:,-1, np.newaxis].to(torch.float32)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward pass\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # calculate loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # write to tensorboard\n",
    "#         writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "#         # backward pass\n",
    "#         loss.backward()\n",
    "\n",
    "#         # optimizer step\n",
    "#         optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.makedirs(\"saved_models\", exist_ok=True)\n",
    "# torch.save(model.state_dict(), \"saved_models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t  = torch.Tensor(test_loader.dataset)\n",
    "\n",
    "# # splid in inputs and labels\n",
    "# test_inputs = t[:,:-1]#.to(torch.float32)\n",
    "# test_labels = t[:,-1, np.newaxis]#.to(torch.float32)\n",
    "\n",
    "# test_results = model(test_inputs).detach()\n",
    "\n",
    "# # see how many percnet where predicted right\n",
    "# threshold = 0.5\n",
    "# ((test_results>threshold)==test_labels).sum()/np.prod(test_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# RocCurveDisplay.from_predictions(\n",
    "#    test_labels.flatten(), test_results.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "buildings = []\n",
    "sat_images = []\n",
    "building_masks = []\n",
    "\n",
    "for city in cities: \n",
    "    buildings.append(datahandler.get_buildings(city))\n",
    "    sat_images.append(datahandler.get_satellite_image(city))\n",
    "    building_masks.append(datahandler.get_building_mask(city))\n",
    "\n",
    "# Plot the expected results for the first city \n",
    "datahandler.plot(city[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preparation\n",
    "\n",
    "for city in cities:\n",
    "    data_preparation.create_tensor(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download \n",
    "\n",
    "for city in cities: \n",
    "    sat_image = datahandler.get_satellite_image(city)\n",
    "    mask = datahandler.get_building_mask(city)\n",
    "\n",
    "# Plot the expected results for the first city \n",
    "datahandler.plot(city[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
