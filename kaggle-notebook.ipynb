{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":8858480,"datasetId":5299351,"databundleVersionId":9018413}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install cloud-tpu-client https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.13-cp38-cp38m-linux_x86_64.whl\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyrosm openeo lightning rasterio plotly","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:32:11.671287Z","iopub.execute_input":"2024-07-04T15:32:11.671563Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting pyrosm\n  Downloading pyrosm-0.6.2.tar.gz (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-","output_type":"stream"}]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"import logging\nfrom pyrosm.data import sources\nimport numpy as np\n\ndef setup_logger(level: int = logging.INFO):\n    \"\"\"\n    Set up a logger for the pipeline. \n    \"\"\"\n    logger = logging.getLogger()\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s\"\n    )\n    \n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatter)\n    \n    file_handler = logging.FileHandler(\"main.log\")\n    file_handler.setFormatter(formatter)\n\n    logger.setLevel(level)\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n\n    return logger\n\n\ndef get_available_cities():\n    \"\"\"\n    Return all available cities from pyrosm \n    \"\"\"\n    return sources.cities.available\n\n\ndef stretch_hist(band):\n    \"\"\"\n    Apply histogram stretching\"\"\"\n    p2, p98 = np.percentile(band, (0.5, 99.5))\n    return np.clip((band - p2) * 255.0 / (p98 - p2), 0, 255).astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:23:04.406150Z","iopub.execute_input":"2024-07-04T15:23:04.407029Z","iopub.status.idle":"2024-07-04T15:23:05.427908Z","shell.execute_reply.started":"2024-07-04T15:23:04.406984Z","shell.execute_reply":"2024-07-04T15:23:05.426854Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# data_acquisition","metadata":{}},{"cell_type":"code","source":"#basics\nimport os\nimport time\nimport json\nimport pickle\nimport openeo\nimport numpy as np\n\n# geography\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.features import geometry_mask\n\n\n#download\nimport pyrosm as pyr\nfrom openeo.rest import OpenEoApiError\nfrom openeo.processes import ProcessBuilder, if_, is_nan\n\n\n\n# plotting \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\n\n\n\nclass DataHandler: \n    def __init__(self, logger, path_to_data_directory = \"data\"):\n        \"\"\"\n        Initialize the DataHandler class and define openeo params.\n        \"\"\"\n        self.logger = logger\n        self.openeo_temporal_extent = [\"2023-05-01\", \"2023-09-30\"]\n        self.openeo_bands = [\"B04\", \"B03\", \"B02\", \"B08\", \"B12\", \"B11\", \"SCL\"]\n        self.openeo_max_cloud_cover = 30\n        self.openeo_spatial_resolution = 10\n        self.openeo_connection = None\n        self.openeo_collections = None\n        self.openeo_jobs = None#\n        self.path_to_data_directory = path_to_data_directory\n        \n        \n        if not os.path.exists(self.path_to_data_directory):\n            os.makedirs(self.path_to_data_directory)\n            logger.info(\"Created data directory\")\n        else:\n            logger.info(\"Data directory already exists\")\n\n\n    def create_directory(self, city: str):\n        \"\"\"\n        Create a directory for each city.\n        \"\"\"\n        os.makedirs(os.path.join(self.path_to_data_directory, city), exist_ok=True)\n        self.logger.info(f\"{city}: Directory available\")\n\n\n    def get_buildings(self, city: str):\n        \"\"\"\n        Return buildings for a given city\n        \"\"\"\n        self.create_directory(city)\n        \n        # Check if local data for city is available\n        if \"buildings.geojson\" in os.listdir(os.path.join(self.path_to_data_directory, city)):\n            self.logger.info(f\"{city}: Using local building data\")\n            return gpd.read_file(os.path.join(self.path_to_data_directory, city,\"buildings.geojson\"))\n\n        # Download data for city\n        fp = pyr.get_data(city, directory=os.path.join(self.path_to_data_directory, city))\n        osm = pyr.OSM(fp)\n        self.logger.info(f\"{city}: Downloaded data to {self.path_to_data_directory}/{city}\")\n\n        # Get bounding box for city\n        boundingbox = self.get_boundingbox(city, osm)\n\n        # Get the buildings of the city\n        buildings_geodf = osm.get_buildings()\n\n        # Remove buildings outside of the bounding box of the city\n        buildings_geodf = buildings_geodf.cx[boundingbox[0] : boundingbox[2], boundingbox[1] : boundingbox[3]]\n\n        # Save the data of the city\n        buildings_path = os.path.join(self.path_to_data_directory, city,\"buildings.geojson\")\n        buildings_geodf.to_file(buildings_path, driver=\"GeoJSON\")\n        self.logger.info(f\"{city}: Stored data to {buildings_path}\")\n\n        return buildings_geodf\n\n\n    def get_boundingbox(self, city: str, osm = None):\n        \"\"\"\n        Get the bounding box for a city.\n        \"\"\"\n\n        # Return bounding box for Berlin as specified in exercise sheet to ensure correct testing results\n        if city == \"Berlin\":\n            return [13.294333, 52.454927, 13.500205, 52.574409]\n\n        # Check if local bounds are available\n        bounds_path = os.path.join(self.path_to_data_directory, city,\"bounds.pkl\")\n        if os.path.exists(bounds_path):\n            with open(bounds_path, \"rb\") as f:\n                boundingbox = pickle.load(f)\n            return boundingbox\n        \n        # Ensure OSM data is available \n        if osm is None:\n            self.get_buildings(city=city)\n\n        # Get the boundaries\n        geoframe_bounds = osm.get_boundaries()\n        boundingbox = geoframe_bounds[geoframe_bounds[\"name\"] == city].total_bounds\n\n        # Check if bounding box is None\n        if np.isnan(boundingbox[0]) or np.isnan(boundingbox[1]) or np.isnan(boundingbox[2]) or np.isnan(boundingbox[3]):\n            self.logger.info(f\"{city}: Bounding box is None. Using total bounds instead\")\n            boundingbox = geoframe_bounds.total_bounds\n        self.logger.info(f\"{city}: Bounding box is {boundingbox}\")     \n\n        # Save total bounds to pickle file\n        with open(bounds_path, \"wb\") as f:\n            pickle.dump(boundingbox, f)\n        self.logger.info(f\"{city}: Saved bounds to {bounds_path}\")\n\n        return boundingbox\n    \n\n    def get_satellite_image(self, city: str, return_rasterio_dataset = False): \n        \"\"\"\n        Get satellite images for a city. Use local data if available. Returns an Array with (H, W, C) shape\n        \"\"\"\n        if os.path.exists(os.path.join(self.path_to_data_directory, city,\"openEO.tif\")):\n            self.logger.info(f\"{city}: Using local satellite image\")\n            ds = rasterio.open(os.path.join(self.path_to_data_directory, city,\"openEO.tif\"))\n            if return_rasterio_dataset:\n                return ds\n            \n            # Read all channels\n            sat_data = ds.read()\n\n            # Transpose to (H, W, C)\n            sat_data = np.transpose(sat_data, (1, 2, 0))\n            return sat_data\n        else:\n            self.download_satellite_image(city)\n            return self.get_satellite_image(city)\n    \n\n    def connect_to_openeo(self):\n        \"\"\"\n        Connect to the openEO backend and \n        \"\"\"\n        if self.openeo_connection is None:\n            connection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\n            connection.authenticate_oidc()\n            self.openeo_connection = connection\n\n            self.logger.info(\"Connected to openEO\")\n        else:\n            self.logger.info(\"Already connected to openEO\")\n\n\n    def download_satellite_image(self, city: str):\n        \"\"\"\n        Download satellite images for a city. Retry for 3 times if the job fails or takes longer than 30 min per job.\n        \"\"\"\n        self.connect_to_openeo()\n        \n        # Log the currently running jobs\n        self.logger.info(\"Current jobs:\")\n        for idx, job in enumerate(self.openeo_connection.list_jobs()):\n            self.logger.info(f\"{idx} {job['id']} {job['status']}\")\n\n        # Retry job up to 3 times. Raise exception after 3 retries.\n        job_finished = False\n        job_number_of_retries = 0\n        while not job_finished : \n            if job_number_of_retries > 3:\n                self.logger.error(f\"{city}: Job failed after 3 retries\")\n                raise Exception(f\"{city}: Job failed after 3 retries\")\n            job = self.create_and_start_openeo_job(city)    \n            job_finished = self.await_job(city, job)\n            job_number_of_retries += 1\n\n        # Get job results and store in data/city\n        job_results = self.openeo_connection.job(job.job_id).get_results()\n        job_results.download_files(os.path.join(self.path_to_data_directory, city))\n        self.logger.info(f\"{city}: Downloaded job results to {os.path.join(self.path_to_data_directory, city)}\")\n\n\n    def delete_jobs(self):\n        \"\"\"\n        Delete all jobs on the openEO backend. Use only for debugging. \n        \"\"\"\n        self.connect_to_openeo()\n\n        for idx, job in enumerate(self.openeo_connection.list_jobs()):\n            self.logger.info(f\"Deleting job {idx}, {job['id']}, {job['status']}\")\n            self.openeo_connection.job(job[\"id\"]).delete_job()\n\n\n    def create_and_start_openeo_job(self, city: str, collection_id: str = \"SENTINEL2_L2A\"):\n        \"\"\"\n        Creates an openeo processing job for a city and starts it.\n        \"\"\"\n        # Transform order in boundingbox to dict\n        boundingbox = self.get_boundingbox(city)\n        boundingbox = {\"west\": boundingbox[0], \"south\": boundingbox[1], \"east\": boundingbox[2], \"north\": boundingbox[3]}\n        \n        # Create datacube\n        datacube = self.openeo_connection.load_collection(\n            collection_id=collection_id,\n            spatial_extent=boundingbox,\n            temporal_extent=self.openeo_temporal_extent,\n            bands=self.openeo_bands,\n            max_cloud_cover=self.openeo_max_cloud_cover,\n        ).resample_spatial(self.openeo_spatial_resolution)\n\n        # Create cloud mask\n        scl = datacube.band(\"SCL\")\n\n        # Filter out cloud median probability, cloud high probability, and snow/ice\n        mask = (scl == 8) | (scl == 9) | (scl == 11)\n\n        # Resample mask to the spatial resolution of the datacube\n        mask = mask.resample_cube_spatial(datacube.band(\"B04\"))\n        \n        # Create the RGB image\n        datacube_rgbFU = datacube.filter_bands(self.openeo_bands[:-1])\n        \n        # Apply cloud mask\n        datacube_rgb_masked = datacube_rgbFU.mask(mask)\n        \n        # Reduce temporal to median \n        datacube_rgb_masked_reduced_t = datacube_rgb_masked.reduce_temporal(\"median\")\n\n        # Define image format \n        datacube_for_submission = datacube_rgb_masked_reduced_t.save_result(format=\"GTiff\")\n        \n        # Create openEO job with datacube\n        job = datacube_for_submission.create_job(title=f\"{city}__pic\")\n        self.logger.info(f\"{city}: Created openEO job\")\n\n        # Start openEO job\n        job.start_job()\n        self.logger.info(f\"{city}: Started openEO job with ID: {job.job_id}\")        \n\n        return job\n\n\n    def await_job(self, city, job):\n        \"\"\"\n        Awaits the processing of a openeo job. \n        Returns when the job is finished or raises an exception if the job failed.\n        \"\"\"\n\n        for i in range(30):\n            status = self.openeo_connection.job(job.job_id).status()\n            self.logger.debug(f\"{city}: Job {job.job_id} status: {status}\")\n          \n            if status == \"finished\":\n                self.logger.info(f\"{city}: Job {job.job_id} finished\")\n                return True\n            \n            elif status == \"error\":\n                self.logger.warning(f\"{city}: Job {job.job_id} failed. Trying again.\")\n                return False            \n            \n            time.sleep(60)\n        self.logger.error(f\"{city}: Job {job.job_id} did not finish in time\")\n        return False\n\n    def get_building_mask(self, city: str, loaded_buildings = None, all_touched: bool = False):  \n        \"\"\"\n        Get the local building mask for buildings in a city.\n        \"\"\"\n        if all_touched:\n            filename = \"building_mask_dense\"\n        else:\n            filename = \"building_mask_sparse\"\n        # Check if the building mask is already available\n        if os.path.exists(os.path.join(self.path_to_data_directory, city,f\"{filename}.tif\")):\n            self.logger.info(f\"{city}: Using local building mask\")\n            return rasterio.open(os.path.join(self.path_to_data_directory, city,f\"{filename}.tif\")).read(1)\n\n        # Create new building mask \n        satellite_image = self.get_satellite_image(city, return_rasterio_dataset=True)\n\n        # Get satellite image metadata\n        transform = satellite_image.transform\n        out_shape = (satellite_image.height, satellite_image.width)\n        crs = satellite_image.crs\n\n        # Read the GeoJSON file with building polygons\n        if loaded_buildings is not None:\n            buildings = loaded_buildings\n        else:\n            buildings = self.get_buildings(city)\n            buildings = buildings.to_crs(crs)  # Ensure the CRS matches the GeoTIFF\n\n        # Create a mask where pixels inside buildings are True, others are False\n        # TODO all_touched paramer nutzen für zweite Maske\n        mask = geometry_mask(\n            buildings.geometry, transform=transform, invert=True, out_shape=out_shape, all_touched=all_touched,\n        )\n        \n        # Store the mask as a GeoTIFF file\n        \n        out_meta = satellite_image.meta\n        out_meta.update(\n            {\n                \"driver\": \"GTiff\",\n                \"height\": mask.shape[0],\n                \"width\": mask.shape[1],\n                # \"transform\": transform,\n                \"count\": 1,\n            }\n        )\n\n        # boolmask is automatically being saved as int16 [0,1]\n  \n        with rasterio.open(os.path.join(self.path_to_data_directory, city,f\"{filename}.tif\"), \"w\", **out_meta) as dest:\n            dest.write(mask, indexes=1)\n\n        return mask\n\n\n\n    def plot(self, city: str = \"BerlinTest\", \n\n             backend: str = \"matplotlib\",\n             figure_size: tuple = (10, 10),\n             brightness: int = 5,\n             image_directory: str = \"img/\",\n             show_plot: bool = False,\n             slice_to_be_plotted = None\n             ):\n        \"\"\"\n        Plot the data for a city either with matplotlib or plotly.\n        \"\"\"\n\n    \n    \n        if backend != \"plotly\" and backend != \"matplotlib\":            \n            raise NotImplementedError(\"Only matplotlib and plotly is supported at the moment\")\n        \n        satellite_data = self.get_satellite_image(city)        \n        mask = self.get_building_mask(city)\n        # Take out slice if only a slice is to be plotted\n        if slice_to_be_plotted is not None:\n            satellite_data = satellite_data[slice(*slice_to_be_plotted)]\n            mask = mask[slice(*slice_to_be_plotted)]\n        \n        if backend ==\"matplotlib\":\n            #load buildings\n            buildings = self.get_buildings(city)\n\n            # create image out path\n            image_path_out = os.path.join(image_directory, city)\n             # make the output directory if not exists\n            os.makedirs(image_path_out, exist_ok=True)\n\n            # Design plots\n            fig, ax = plt.subplots(figsize=figure_size)\n            buildings.plot(ax=ax, color=\"black\")\n            plt.title(f\"{city} buildings\")\n            plt.axis(\"off\")\n\n        # RGB Bands from Sentinel 2\n        red = satellite_data[...,0]\n        green = satellite_data[...,1]\n        blue = satellite_data[...,2]\n\n        # Apply histogram stretching\n        red_stretched = stretch_hist(red)\n        green_stretched = stretch_hist(green)\n        blue_stretched = stretch_hist(blue)\n\n        # Stack the bands after stretching\n        rgb_stretched = np.dstack((red_stretched, green_stretched, blue_stretched))\n\n        \n\n        if backend ==\"matplotlib\":\n            # Plot the histogram-stretched RGB image\n            plt.figure(figsize=figure_size)\n            plt.imshow(rgb_stretched)\n            # plt.title(\"Histogram Stretched RGB Composite Image\")\n            plt.title(f\"{city} RGB Bands from Sentinel-2 L2A\")\n            plt.axis(\"off\")\n            # plt.show()\n            plt.savefig(os.path.join(image_path_out, f\"{city}_RGB.png\"))\n            if show_plot:\n                plt.show()\n            plt.close()\n\n\n        # RGB image with higher brightness\n        red_norm = (red - np.min(red)) / (np.max(red) - np.min(red))\n        green_norm = (green - np.min(green)) / (np.max(green) - np.min(green))\n        blue_norm = (blue - np.min(blue)) / (np.max(blue) - np.min(blue))\n        pseudo_RGB_image = np.dstack((red_norm, green_norm, blue_norm))\n\n        pseudo_RGB_image_normalized = (pseudo_RGB_image - np.min(pseudo_RGB_image)) / (\n            pseudo_RGB_image.max() - pseudo_RGB_image.min()\n        )\n\n\n        pseudo_RGB_image_brighter = pseudo_RGB_image_normalized * brightness\n        pseudo_RGB_image_brighter = np.clip(pseudo_RGB_image_brighter, 0, 1)\n\n        if backend ==\"matplotlib\":\n            plt.figure(figsize=figure_size)\n            plt.imshow(pseudo_RGB_image_brighter)\n            plt.title(f\"{city} RGB Image\")\n            plt.axis(\"off\")\n            # plt.show()\n            plt.savefig(os.path.join(image_path_out, f\"{city}_RGB_Brighter.png\"))\n            if show_plot:\n                plt.show()\n            plt.close()\n\n            # single band img\n            # single_band = satellite_image.read(1)\n            single_band_stretched = stretch_hist(red)\n            plt.figure(figsize=figure_size)\n            plt.imshow(single_band_stretched, cmap=\"gray\")\n            plt.title(f\"{city} Single Band Image\")\n            plt.axis(\"off\")\n            # plt.show()\n            plt.savefig(os.path.join(image_path_out, f\"{city}_SingleBand.png\"))\n            if show_plot:\n                plt.show()\n            plt.close()\n        elif backend == \"plotly\":\n\n            # plot the mask\n            fig = px.imshow(mask.astype(np.uint8), binary_string=True)\n\n            # Overlay the mask with the image\n            fig.add_trace(go.Image(z=(pseudo_RGB_image_brighter * 255).astype(np.uint8), opacity=1))\n\n\n            # Update layout with a button to toggle mask visibility\n            fig.update_layout(\n                updatemenus=[\n                    dict(\n                        type=\"buttons\",\n                        direction=\"left\",\n                        buttons=list([\n                            dict(\n                                args=[{\"opacity\": [0,1]}],\n                                label=\"Hide Mask\",\n                                method=\"restyle\"\n                            ),\n                            dict(\n                                args=[{\"opacity\": [0.5, 0.5]}],\n                                label=\"Show Mask\",\n                                method=\"restyle\"\n                            )\n                        ]),\n                    ),\n                ],\n                xaxis=dict(\n                    scaleanchor=\"y\",\n                    scaleratio=1\n                ),\n                yaxis=dict(\n                    scaleanchor=\"x\",\n                    scaleratio=1\n                )\n            )\n\n            # Enable zooming and panning\n            fig.update_xaxes(constrain='domain')\n            fig.update_yaxes(scaleanchor='x', scaleratio=1)\n            fig.update_layout(height=1000, width=1000)\n\n            # Display the figure\n            return fig\n\n\n        # B8 B4 B3 -> False Color\n        b8 = satellite_data[...,3]\n        b8_stretched = stretch_hist(b8)\n        b4 = red_stretched\n        b3 = green_stretched\n\n        false_color = np.dstack((b8_stretched, b4, b3))\n        plt.figure(figsize=figure_size)\n        plt.imshow(false_color)\n        plt.title(f\"{city} False Color Image\")\n        plt.axis(\"off\")\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_FalseColor.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n        # params[\"bands\"] = [\"B04\", \"B03\", \"B02\", \"B08\", \"B12\", \"B11\", \"SCL\"] # scl must be last\n\n        # B12, B11, B4 -> False Color Urban\n        b12 = satellite_data[...,4]\n        b11 = satellite_data[...,5]\n        b04 = red\n        b12_norm = (b12 - np.min(b12)) / (np.max(b12) - np.min(b12))\n        b11_norm = (b11 - np.min(b11)) / (np.max(b11) - np.min(b11))\n        b04_norm = (b04 - np.min(b04)) / (np.max(b04) - np.min(b04))\n\n\n        false_color_urban = np.dstack((b12_norm, b11_norm, b04_norm)) * brightness\n        false_color_urban = np.clip(false_color_urban, 0, 1)\n\n        plt.figure(figsize=figure_size)\n        plt.imshow(false_color_urban)\n        plt.title(f\"{city} False Color Urban Image\")\n        plt.axis(\"off\")\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_FalseColorUrban.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n\n        # get vegetation_index\n        def vegetation_index(band1, band2):\n            return (band1 - band2) / (band1 + band2)\n\n\n        ndvi = vegetation_index(satellite_data[...,3], satellite_data[...,2])\n        plt.figure(figsize=figure_size)\n        plt.imshow(ndvi, cmap=\"RdYlGn\")\n        plt.title(f\"{city} NDVI Image\")\n        plt.axis(\"off\")\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_NDVI.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n        # Visualize the mask\n        plt.figure(figsize=(10, 10))\n        plt.imshow(mask, cmap=\"Blues\")\n        plt.title(f\"{city} Building Mask\")\n        plt.axis(\"off\")\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_BuildingMask.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n\n        # Load the image\n        img = single_band_stretched  # Assuming `blue_stretched` is the single band image\n        blue_cmap = plt.cm.Blues\n        blue_building_mask = blue_cmap(mask / mask.max())\n        blue_building_mask[..., 2] = mask * 0.8\n\n        # Plot the image\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img, cmap=\"gray\", alpha=1)\n\n        plt.imshow(blue_building_mask)\n\n        # Set the title and axis labels\n        plt.title(f\"{city} Image with Buildings Mask\")\n        plt.axis(\"off\")\n\n        # Show the plot\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_BuildingMaskOverlay.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:23:05.429505Z","iopub.execute_input":"2024-07-04T15:23:05.429956Z","iopub.status.idle":"2024-07-04T15:23:06.711149Z","shell.execute_reply.started":"2024-07-04T15:23:05.429930Z","shell.execute_reply":"2024-07-04T15:23:06.710376Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# data_preperation","metadata":{}},{"cell_type":"code","source":"import rasterio\nimport numpy as np\nimport torch\nfrom torch.utils.data import random_split, DataLoader, Dataset\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import kl_div\nimport pandas as pd\nimport logging\n\ndef create_tensor_of_windows(image, mask, patch_size=None):\n    \"\"\"\n    Create tensor with dimensions [N, H, W, C+1] from the satellite image of the city.\n    image should be of shape (H, W, C)\n    mask should be of shape (H, W, 1)\n    \"\"\"\n\n    # Merge Mask onto Image\n    image_with_mask = np.dstack((image, mask))\n\n    # cut of edges so image shape is divisible by patch size\n    reduced_image = image_with_mask[:-(image_with_mask.shape[0]%patch_size), :-(image_with_mask.shape[1]%patch_size)]\n\n    # get reduced image size\n    orig_img_x_size = reduced_image.shape[0]\n    orig_img_y_size = reduced_image.shape[1]\n\n    # get factors to which each dimension is reduced\n    down_scale_x = orig_img_x_size//patch_size\n    down_scale_y = orig_img_y_size//patch_size\n\n    # create array for pathched images\n    patched_images = np.zeros([down_scale_x*down_scale_y, patch_size, patch_size, reduced_image.shape[2]], dtype=np.uint16)\n\n    # fill array with patches\n    for i in range(down_scale_x):\n        for j in range(down_scale_y):\n            patched_images[i*down_scale_x+j::down_scale_x*down_scale_y] = reduced_image[patch_size*i:patch_size*(i+1), patch_size*j:patch_size*(j+1)]\n\n    # return array\n    return patched_images\n\n   \ndef divide_into_test_training(data, test_ratio=0.2, validation_ratio=0, seed=42):\n    \"\"\"\n    Divide the data into test and training split with seed.\n    \"\"\"\n    \n    # Define the split rati\n    train_ratio = 1 - test_ratio - validation_ratio\n    if train_ratio < 0:\n        raise ValueError(\"The train ratio is negative. Please check the split ratios.\")\n\n    # # Calculate the sizes for training and test sets\n    # train_size = int(train_ratio * len(data))\n    # test_size = int(test_ratio * len(data))\n    # validation_size = int(validation_ratio * len(data))\n\n    # Split the dataset with seed\n    generator = torch.Generator().manual_seed(seed)\n    train_dataset, test_dataset, validation_dataset = random_split(data, [train_ratio, test_ratio, validation_ratio], generator=generator)\n\n    return train_dataset, test_dataset, validation_dataset\n\n\ndef validate_test_training_validation_split(train_dataset, test_dataset, validation_dataset, city_names=None):\n    \"\"\"\n    Validate the train to the test split and the train to the validation split.\n    \"\"\"\n    logger = logging.getLogger()\n    # take out dataset for better readabiltiy\n    dataset = train_dataset.dataset[:,:-2]\n\n    # calculate mean, std, min and max for each image\n    means = dataset.mean(axis=(2,3))\n    stds = dataset.std(axis=(2,3))\n    mins = dataset.min(axis=(2,3))\n    maxs = dataset.max(axis=(2,3))\n\n    # create dataframes for train, test and validation set\n    train_means = pd.DataFrame({\n        \"mean\":means[train_dataset.indices].mean(axis=0), \n        \"std\":stds[train_dataset.indices].mean(axis=0),\n        \"min\":mins[train_dataset.indices].mean(axis=0),\n        \"max\":maxs[train_dataset.indices].mean(axis=0),\n        }, index=[\"R\", \"G\", \"B\",\"B08\", \"B12\", \"B11\"])\n\n    test_means = pd.DataFrame({\n        \"mean\":means[test_dataset.indices].mean(axis=0), \n        \"std\":stds[test_dataset.indices].mean(axis=0),\n        \"min\":mins[test_dataset.indices].mean(axis=0),\n        \"max\":maxs[test_dataset.indices].mean(axis=0),\n        }, index=[\"R\", \"G\", \"B\",\"B08\", \"B12\", \"B11\"])\n\n    # test if differences between train and test set are below 10%\n    if (((train_means-test_means)/train_means)<0.1).all().all():\n        print(u'\\u2713',\"Differences of train and test set is below 10% on mean, std, min and max across all input bands\",)\n    else:\n        # if not show differences and give out Warning\n        temp_df =(train_means-test_means)/train_means\n        logger.warning(\"Differences of train and test set is above 10% on one of mean, std, min and max across all input bands. This might be too big of a difference between train and test set. Please choose another seed for splitting.\")\n        print(\"!!!There might be large diffferecenes between train and test set. Please choose another seed for splitting. For more detail see the differences below\",)\n        print(temp_df[temp_df>0.1].dropna(axis=1, how='all').dropna(axis=0, how='all'))\n        \n    if validation_dataset.indices:\n        validation_means = pd.DataFrame({\n            \"mean\":means[validation_dataset.indices].mean(axis=0), \n            \"std\":stds[validation_dataset.indices].mean(axis=0),\n            \"min\":mins[validation_dataset.indices].mean(axis=0),\n            \"max\":maxs[validation_dataset.indices].mean(axis=0),\n            }, index=[\"R\", \"G\", \"B\",\"B08\", \"B12\", \"B11\"])\n        \n        # test if differences between train and validation set are below 10%\n        if (((train_means-validation_means)/train_means)<0.1).all().all():\n            print(u'\\u2713',\"Differences of train and validation set is below 10% on mean, std, min and max across all input bands\",)\n        else:\n            # if not show differences and give out Warning\n            temp_df =(train_means-validation_means)/train_means\n            logger.warning(\"Differences of train and validation set is above 10% on one of mean, std, min and max across all input bands. This might be too big of a difference between train and test set. Please choose another seed for splitting.\")\n            print(\"!!!There might be large diffferecenes between train and validation set. Please choose another seed for splitting. For more detail see the differences below\",)\n            print(temp_df[temp_df>0.1].dropna(axis=1, how='all').dropna(axis=0, how='all'))\n\n    print()\n    # Look at distribution of masks\n    masks  = train_dataset.dataset[:,[-2]]\n\n    # sum the pixels of building up over each image\n    sum = masks.sum(axis=(2,3))\n\n    # create dataframes for train, test and validation set with descriptive statistics\n    train_means_labels = pd.DataFrame({\n        \"mean\":sum[train_dataset.indices].mean(axis=0), \n        \"median\":np.median(sum[train_dataset.indices],axis=0), \n        \"std\":sum[train_dataset.indices].std(axis=0),\n        \"10th percentile\":np.percentile(sum[train_dataset.indices], q=10,axis=0),\n        \"90th percentile\":np.percentile(sum[train_dataset.indices], q=90,axis=0),\n        }, index=[\"Train\"])\n    test_means_labels = pd.DataFrame({\n        \"mean\":sum[test_dataset.indices].mean(axis=0),\n        \"median\":np.median(sum[test_dataset.indices],axis=0), \n        \"std\":sum[test_dataset.indices].std(axis=0),\n        \"10th percentile\":np.percentile(sum[test_dataset.indices], q=10,axis=0),\n        \"90th percentile\":np.percentile(sum[test_dataset.indices], q=90,axis=0),\n        }, index=[\"Test\"])\n    if validation_dataset.indices:\n        validation_means_labels = pd.DataFrame({\n            \"mean\":sum[validation_dataset.indices].mean(axis=0), \n            \"median\":np.median(sum[validation_dataset.indices],axis=0), \n            \"std\":sum[validation_dataset.indices].std(axis=0),\n            \"10th percentile\":np.percentile(sum[validation_dataset.indices], q=10,axis=0),\n            \"90th percentile\":np.percentile(sum[validation_dataset.indices], q=90,axis=0),\n            }, index=[\"Validation\"])\n        # print concatenated dataframes\n        print(\"Comparison of distribution of masks:\\n\",pd.concat([train_means_labels, test_means_labels, validation_means_labels]))\n    else:\n        print(\"Comparison of distribution of masks:\\n\", pd.concat([train_means_labels, test_means_labels] ))\n\n    print()\n\n    # look at the distribution of the data according to the different cities\n    if city_names is not None:  \n        # take out the city name for each label\n        train_cities = train_dataset.dataset[:,-1,0,0][train_dataset.indices]\n        test_cities = test_dataset.dataset[:,-1,0,0][test_dataset.indices]\n        validation_cities = validation_dataset.dataset[:,-1,0,0][validation_dataset.indices]\n\n        # lookup how often each city occured in the different sets\n        train_city_counts = np.unique(train_cities, return_counts=True)\n        test_city_counts = np.unique(test_cities, return_counts=True)\n        validation_citiy_counts = np.unique(validation_cities, return_counts=True)\n\n        # create dataframes for better readability\n        df = pd.DataFrame({\n            \"Train\":pd.Series(train_city_counts[1]/train_cities.shape[0], index=train_city_counts[0], name='train'),\n            \"Test\":pd.Series(test_city_counts[1]/test_cities.shape[0], index=test_city_counts[0], name='test'),\n            \"Validation\":pd.Series(validation_citiy_counts[1]/validation_cities.shape[0], index=validation_citiy_counts[0], name='validation')})\n        print(df.index)\n        df.index = df.index.map({i:c for i ,c in enumerate(city_names)})\n        print(\"Comparison of cities the data in the differen sets originates from:\\n\",df.T)\n\n\n\n\ndef create_data_loaders(train_dataset, test_dataset, validation_dataset, batch_size = 64):\n    \"\"\"\n    Create DataLoaders.\n    \"\"\"\n    # Create DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, test_loader, validation_loader\n\n\ndef apply_preprocessing_pipeline(images, masks, patch_size = 128, test_ratio = 0.2,validation_ratio=0, batch_size = 64, show_validation_of_split=True, city_names=None, minimum_number_of_true_pixels_per_image=0):\n    \"\"\"\n    applies windowing, deviding into train and test and creating data loaders.\n    \"\"\"\n\n    # for each city create patched images\n    patched_images = []\n    for i,(image, mask ) in enumerate(zip(images, masks)):\n        patched_image = create_tensor_of_windows(image, mask, patch_size=patch_size)\n        city = np.ones(shape=list(patched_image.shape[:-1])+[1])*i\n        patched_image_with_city = np.concatenate([patched_image, city], axis=-1)\n        patched_images.append(patched_image_with_city)\n\n\n    # concatenate all patched images\n    patched_images_merged = np.concatenate(patched_images, axis=0)\n\n    # reorder axis to [N, C, H, W] for torch\n    patched_images_merged = np.transpose(patched_images_merged, (0,3,1,2))\n    \n    # discard images with less than minimum_number_of_true_pixels_per_image\n    sums = patched_images_merged[:,-2].sum(axis=(1,2))\n    patched_images_merged = patched_images_merged[sums>=minimum_number_of_true_pixels_per_image]\n\n    # devide into train and test\n    train_dataset, test_dataset, validation_dataset = divide_into_test_training(patched_images_merged, test_ratio=test_ratio, validation_ratio=validation_ratio)\n\n    if show_validation_of_split:\n        validate_test_training_validation_split(train_dataset, test_dataset, validation_dataset, city_names=city_names)\n\n    dataset = train_dataset.dataset[:,:-1]\n    train_dataset.dataset = dataset\n    test_dataset.dataset = dataset\n    validation_dataset.dataset = dataset    \n    # create data loaders\n    train_loader, test_loader , validation_loader= create_data_loaders(train_dataset, test_dataset,validation_dataset, batch_size=batch_size)\n\n    # TODO fix error\n    # TODO remove city names\n\n    return train_loader, test_loader, validation_loader\n\n\ndef plot_sub_image( image_data):\n    \"\"\"\n    Plot sub image.\n    \"\"\"\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    ax[0].imshow(stretch_hist(image_data[:,:,:3]))\n    ax[1].imshow(stretch_hist(image_data[:,:,-1]))\n    return fig\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:23:06.714111Z","iopub.execute_input":"2024-07-04T15:23:06.715000Z","iopub.status.idle":"2024-07-04T15:23:10.168419Z","shell.execute_reply.started":"2024-07-04T15:23:06.714960Z","shell.execute_reply":"2024-07-04T15:23:10.167590Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"# basics\nimport os\n\nimport numpy as np\nfrom tqdm.notebook import tqdm \n\n\n# torch\nimport torch\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader\nimport lightning as L\n\n\n\n\n# custom modules\n\n\n\n\n# Configure logging for the pipeline\nlogger = setup_logger(level='ERROR')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:23:10.169635Z","iopub.execute_input":"2024-07-04T15:23:10.170181Z","iopub.status.idle":"2024-07-04T15:23:25.212775Z","shell.execute_reply.started":"2024-07-04T15:23:10.170153Z","shell.execute_reply":"2024-07-04T15:23:25.211699Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-04 15:23:12.221730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-04 15:23:12.221835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-04 15:23:12.351650: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"cities = ['London', 'CapeTown', 'Hamburg', 'Johannesburg', 'London', 'Montreal', 'Paris', 'Seoul', 'Singapore', 'Sydney']\n\ndatahandler = DataHandler(logger, '/kaggle/input/building-prediction/')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:23:25.214425Z","iopub.execute_input":"2024-07-04T15:23:25.215355Z","iopub.status.idle":"2024-07-04T15:23:25.223117Z","shell.execute_reply.started":"2024-07-04T15:23:25.215291Z","shell.execute_reply":"2024-07-04T15:23:25.221979Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# load images and mask for all specified cites\n\nimport os\nimages = []\nsparse_masks=[]\ndense_masks=[]\n\nfor city in cities:\n    buildings = None\n    if not os.path.exists(os.path.join(datahandler.path_to_data_directory,city,'building_mask_dense.tif')):\n        print(\"loading local buildings\")\n        buildings = datahandler.get_buildings(city)\n    images.append(datahandler.get_satellite_image(city))\n    sparse_masks.append(datahandler.get_building_mask(city, all_touched=False, loaded_buildings=buildings))\n    dense_masks.append(datahandler.get_building_mask(city, all_touched=True, loaded_buildings=buildings))","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:23:25.224629Z","iopub.execute_input":"2024-07-04T15:23:25.225038Z","iopub.status.idle":"2024-07-04T15:24:04.474210Z","shell.execute_reply.started":"2024-07-04T15:23:25.225003Z","shell.execute_reply":"2024-07-04T15:24:04.473311Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n","output_type":"stream"}]},{"cell_type":"code","source":"masks = [sparse_masks[i]+dense_masks[i]for i in range(len(sparse_masks))]","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:24:04.475617Z","iopub.execute_input":"2024-07-04T15:24:04.476434Z","iopub.status.idle":"2024-07-04T15:24:04.651706Z","shell.execute_reply.started":"2024-07-04T15:24:04.476393Z","shell.execute_reply":"2024-07-04T15:24:04.650848Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"patch_size = 128\ntest_ratio= 0.2\nvalidation_ratio=0.2\nbatch_size = 64\nshow_validation_of_split=False\ncity_names=cities\nminimum_number_of_true_pixels_per_image=1","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:24:04.652907Z","iopub.execute_input":"2024-07-04T15:24:04.653219Z","iopub.status.idle":"2024-07-04T15:24:04.658021Z","shell.execute_reply.started":"2024-07-04T15:24:04.653191Z","shell.execute_reply":"2024-07-04T15:24:04.657064Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# for each city create patched images\npatched_images = []\nfor i,(image, mask ) in enumerate(zip(images, masks)):\n    patched_image = create_tensor_of_windows(image, mask, patch_size=patch_size)\n    city = np.ones(shape=list(patched_image.shape[:-1])+[1])*i\n    patched_image_with_city = np.concatenate([patched_image, city], axis=-1)\n    patched_images.append(patched_image_with_city.astype(np.int16))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:24:04.660667Z","iopub.execute_input":"2024-07-04T15:24:04.661034Z","iopub.status.idle":"2024-07-04T15:24:21.669129Z","shell.execute_reply.started":"2024-07-04T15:24:04.660998Z","shell.execute_reply":"2024-07-04T15:24:21.668270Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n                          locals().items())), key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:24:38.816303Z","iopub.execute_input":"2024-07-04T15:24:38.816663Z","iopub.status.idle":"2024-07-04T15:24:38.825534Z","shell.execute_reply.started":"2024-07-04T15:24:38.816636Z","shell.execute_reply":"2024-07-04T15:24:38.824399Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"       patched_image_with_city:  1.0 GiB\n                 patched_image: 224.0 MiB\n                          city: 128.0 MiB\n                          mask: 33.2 MiB\n                           _i3: 40.7 KiB\n                           _i4: 11.4 KiB\n                   DataHandler:  1.2 KiB\n                OpenEoApiError:  1.0 KiB\n                ProcessBuilder:  1.0 KiB\n                    DataLoader:  1.0 KiB\n","output_type":"stream"}]},{"cell_type":"code","source":"sys.getsizeof(patched_images[0])/1e6","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:24:43.815504Z","iopub.execute_input":"2024-07-04T15:24:43.816347Z","iopub.status.idle":"2024-07-04T15:24:43.824844Z","shell.execute_reply.started":"2024-07-04T15:24:43.816286Z","shell.execute_reply":"2024-07-04T15:24:43.823704Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"557.842592"},"metadata":{}}]},{"cell_type":"code","source":"\n# concatenate all patched images\npatched_images_merged = np.concatenate(patched_images, axis=0)\n\n# reorder axis to [N, C, H, W] for torch\npatched_images_merged = np.transpose(patched_images_merged, (0,3,1,2))\n\n# discard images with less than minimum_number_of_true_pixels_per_image\nsums = patched_images_merged[:,-2].sum(axis=(1,2))\npatched_images_merged = patched_images_merged[sums>=minimum_number_of_true_pixels_per_image]\n\n# devide into train and test\ntrain_dataset, test_dataset, validation_dataset = divide_into_test_training(patched_images_merged, test_ratio=test_ratio, validation_ratio=validation_ratio)\n\nif show_validation_of_split:\n    validate_test_training_validation_split(train_dataset, test_dataset, validation_dataset, city_names=city_names)\n\ndataset = train_dataset.dataset[:,:-1]\ntrain_dataset.dataset = dataset\ntest_dataset.dataset = dataset\nvalidation_dataset.dataset = dataset    \n# create data loaders\ntrain_loader, test_loader , validation_loader= create_data_loaders(train_dataset, test_dataset,validation_dataset, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:29:41.954009Z","iopub.execute_input":"2024-07-04T15:29:41.954405Z","iopub.status.idle":"2024-07-04T15:29:44.317956Z","shell.execute_reply.started":"2024-07-04T15:29:41.954369Z","shell.execute_reply":"2024-07-04T15:29:44.316793Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# apply training pipeline\n# TODO make train test split consistent so we can train with multiple sizes, dont know if there is an advantage though\ntrain_loader, test_loader , validation_loader= apply_preprocessing_pipeline(images, masks, patch_size = 128, test_ratio= 0.2,validation_ratio=0.2, batch_size = 64, show_validation_of_split=False,city_names=cities, minimum_number_of_true_pixels_per_image=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T13:09:33.716155Z","iopub.status.idle":"2024-07-04T13:09:33.716510Z","shell.execute_reply.started":"2024-07-04T13:09:33.716345Z","shell.execute_reply":"2024-07-04T13:09:33.716358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class convNetSimple(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n                nn.Conv2d(6, 32, kernel_size=3, padding=1), nn.ReLU(),\n                nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n                nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n                nn.Conv2d(128, 1, kernel_size=1, padding=0),\n                nn.Sigmoid())\n    \n    def forward(self, x):\n        return self.model(x)\n    \nclass LitNet(L.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.loss = nn.BCELoss()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch[:,:-1], batch[:,-1]\n        outs = self.model(x.float())\n        loss = self.loss(outs, y.unsqueeze(1).float())\n        self.log(\"train_loss\", value=loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch[:,:-1], batch[:,-1]\n        outs = self.model(x.float())\n        loss = self.loss(outs, y.unsqueeze(1).float())\n        \n        values = {\n            \"test_loss\": loss,\n        }\n        self.log_dict(values, on_epoch=True, on_step=True, prog_bar=True, logger=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n    \n    def forward(self, x):\n        return self.model(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:27:40.440419Z","iopub.execute_input":"2024-07-04T15:27:40.440806Z","iopub.status.idle":"2024-07-04T15:27:40.454114Z","shell.execute_reply.started":"2024-07-04T15:27:40.440775Z","shell.execute_reply":"2024-07-04T15:27:40.453111Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Load Model","metadata":{}},{"cell_type":"code","source":"# load trained model\nconvmodel = LitNet(convNetSimple())\nmodel_dict = torch.load(\"models/lightning_logs/version_1/checkpoints/epoch=39-step=7000.ckpt\", map_location=torch.device('cpu'))\nconvmodel.load_state_dict(model_dict['state_dict'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n\nimport pytorch_lightning as pl\n\nL.seed_everything(42)\nconvmodel = LitNet(convNetSimple())\ntrainer = L.Trainer(\n    default_root_dir=\"models\",\n    # callbacks=[\n    #     EarlyStopping(\n    #         monitor=\"val_loss\",\n    #         mode=\"min\",\n    #         patience=10,\n    #     )\n    #     ModelCheckpoint(\n    #         monitor=\"val_loss\",\n    #         mode=\"min\",\n    #         save_top_k=2,\n    #         dirpath=\"models\",\n    #         filename=\"best_model\"\n    #     )\n    # ]\n    # val_check_interval=1,\n    fast_dev_run=False,\n    # num_sanity_val_steps=2,\n    max_epochs=300,\n    log_every_n_steps=20,\n    accelerator=\"gpu\", devices=2, strategy=\"ddp_notebook\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:30:44.564893Z","iopub.execute_input":"2024-07-04T15:30:44.565809Z","iopub.status.idle":"2024-07-04T15:30:44.644397Z","shell.execute_reply.started":"2024-07-04T15:30:44.565757Z","shell.execute_reply":"2024-07-04T15:30:44.643434Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n2024-07-04 15:30:44,570 - lightning.fabric.utilities.seed - INFO - seed_everything - Seed set to 42\n2024-07-04 15:30:44,638 - pytorch_lightning.utilities.rank_zero - INFO - _info - GPU available: True (cuda), used: True\n2024-07-04 15:30:44,639 - pytorch_lightning.utilities.rank_zero - INFO - _info - TPU available: False, using: 0 TPU cores\n2024-07-04 15:30:44,640 - pytorch_lightning.utilities.rank_zero - INFO - _info - HPU available: False, using: 0 HPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader.dataset.dataset.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:29:51.489197Z","iopub.execute_input":"2024-07-04T15:29:51.490103Z","iopub.status.idle":"2024-07-04T15:29:51.497375Z","shell.execute_reply.started":"2024-07-04T15:29:51.490067Z","shell.execute_reply":"2024-07-04T15:29:51.496216Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(8762, 7, 128, 128)"},"metadata":{}}]},{"cell_type":"code","source":"\n# training\ntrainer.fit(convmodel, \n    train_dataloaders=train_loader,\n    val_dataloaders=validation_loader\n)\n\n\n# testing\n\n# hier könnte man noch das beste model laden, wenn wir ein Val dataset haben.\n# best_model = LitModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n# trainer.test(\n#     best_model,\n#     dataloaders=test_loader\n# )\n\n\ntrainer.test(\n    convmodel,\n    dataloaders=test_loader\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T15:30:47.368971Z","iopub.execute_input":"2024-07-04T15:30:47.369802Z","iopub.status.idle":"2024-07-04T15:30:50.024057Z","shell.execute_reply.started":"2024-07-04T15:30:47.369766Z","shell.execute_reply":"2024-07-04T15:30:50.022368Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n2024-07-04 15:30:47,561 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name  | Type          | Params | Mode \n------------------------------------------------\n0 | model | convNetSimple | 94.2 K | train\n1 | loss  | BCELoss       | 0      | train\n------------------------------------------------\n94.2 K    Trainable params\n0         Non-trainable params\n94.2 K    Total params\n0.377     Total estimated model params size (MB)\n2024-07-04 15:30:47,682 - lightning.pytorch.callbacks.model_summary - INFO - summarize - \n  | Name  | Type          | Params | Mode \n------------------------------------------------\n0 | model | convNetSimple | 94.2 K | train\n1 | loss  | BCELoss       | 0      | train\n------------------------------------------------\n94.2 K    Trainable params\n0         Non-trainable params\n94.2 K    Total params\n0.377     Total estimated model params size (MB)\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee8c40db530d45338aab9abaafd07c07"}},"metadata":{}},{"name":"stderr","text":"/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [37,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [52,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [39,0,0], thread: [30,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [39,0,0], thread: [31,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [39,0,0], thread: [93,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [115,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [124,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [127,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [5,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [30,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [39,0,0], thread: [117,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [39,0,0], thread: [125,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [39,0,0], thread: [52,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [39,0,0], thread: [53,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [39,0,0], thread: [63,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [69,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [84,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [41,0,0], thread: [95,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [119,0,0], thread: [125,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [119,0,0], thread: [126,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [119,0,0], thread: [61,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [96,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [98,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [100,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [102,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [104,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [107,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [115,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [116,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [118,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [120,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [122,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [123,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [125,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [126,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [200,0,0], thread: [127,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [97,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [99,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [101,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [104,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [105,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [107,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [109,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [115,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [120,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [122,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [124,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [126,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [65,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [68,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [69,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [72,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [77,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [82,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [83,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [88,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [91,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [221,0,0], thread: [94,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1308\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[22], line 23\u001b[0m, in \u001b[0;36mLitNet.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mloss, on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3122\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3120\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_loader\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# testing\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# hier könnte man noch das beste model laden, wenn wir ein Val dataset haben.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     dataloaders=test_loader\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     18\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(\n\u001b[1;32m     19\u001b[0m     convmodel,\n\u001b[1;32m     20\u001b[0m     dataloaders\u001b[38;5;241m=\u001b[39mtest_loader\n\u001b[1;32m     21\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:60\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     59\u001b[0m     _interrupt(trainer, exception)\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_teardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1009\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_teardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m     loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_loop\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:535\u001b[0m, in \u001b[0;36mStrategy.teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: moving model to CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/fabric/utilities/device_dtype_mixin.py:82\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m _update_properties(\u001b[38;5;28mself\u001b[39m, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:967\u001b[0m, in \u001b[0;36mModule.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \n\u001b[1;32m    961\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:967\u001b[0m, in \u001b[0;36mModule.cpu.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \n\u001b[1;32m    961\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"code","source":"# # Instantiate the model, loss function, and optimizer\n# criterion = nn.BCELoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# # Training loop\n# num_epochs = 50\n\n# model.train()\n# for epoch in tqdm(range(num_epochs)):\n#     for batch in train_loader:\n#         # splid in inputs and labels\n#         inputs = batch[:,:-1].to(torch.float32)\n#         labels = batch[:,-1, np.newaxis].to(torch.float32)\n\n#         # zero the parameter gradients\n#         optimizer.zero_grad()\n\n#         # forward pass\n#         outputs = model(inputs)\n\n#         # calculate loss\n#         loss = criterion(outputs, labels)\n\n#         # write to tensorboard\n#         writer.add_scalar(\"Loss/train\", loss, epoch)\n\n#         # backward pass\n#         loss.backward()\n\n#         # optimizer step\n#         optimizer.step()\n    \n# writer.flush()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Model","metadata":{}},{"cell_type":"code","source":"# import os\n\n# os.makedirs(\"saved_models\", exist_ok=True)\n# torch.save(model.state_dict(), \"saved_models/model1\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"convmodel(inputs).detach().shape","metadata":{"execution":{"iopub.status.busy":"2024-07-04T13:48:36.472733Z","iopub.execute_input":"2024-07-04T13:48:36.473640Z","iopub.status.idle":"2024-07-04T13:48:39.494302Z","shell.execute_reply.started":"2024-07-04T13:48:36.473605Z","shell.execute_reply":"2024-07-04T13:48:39.493271Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"torch.Size([64, 1, 128, 128])"},"metadata":{}}]},{"cell_type":"code","source":"prediction = np.ones((len(test_loader.dataset.indices), 1,128, 128))*-1\ntrue_values = np.ones((len(test_loader.dataset.indices), 1, 128,128))*-1\nfor i,batch in enumerate(test_loader):\n    inputs = batch[:,:-1].to(torch.float32)\n    labels = batch[:,-1, np.newaxis].to(torch.float32)\n    prediction[i*64:(i+1)*64]=convmodel(inputs).detach()\n    true_values[i*64:(i+1)*64]=labels","metadata":{"execution":{"iopub.status.busy":"2024-07-04T13:49:24.569609Z","iopub.execute_input":"2024-07-04T13:49:24.570537Z","iopub.status.idle":"2024-07-04T13:50:06.109456Z","shell.execute_reply.started":"2024-07-04T13:49:24.570499Z","shell.execute_reply":"2024-07-04T13:50:06.108534Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"true_values.sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T13:50:40.059461Z","iopub.execute_input":"2024-07-04T13:50:40.061087Z","iopub.status.idle":"2024-07-04T13:50:40.084566Z","shell.execute_reply.started":"2024-07-04T13:50:40.061034Z","shell.execute_reply":"2024-07-04T13:50:40.082730Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"1048021.0"},"metadata":{}}]},{"cell_type":"code","source":"# predict on test set\n\n\nt  = torch.Tensor(test_loader.dataset)\n\n# splid in inputs and labels\ntest_inputs = t[:,:-1]#.to(torch.float32)\ntest_labels = t[:,-1, np.newaxis]#.to(torch.float32)\n\ntest_results = convmodel(test_inputs).detach()\n\n# Look at sums, to check if model only predicts zeros\nprint(\"Sum of test results: \", test_results.sum())\nprint(\"But it should be closer to: \", test_labels.sum())\n\n\n# # see how many percnet where predicted right\nthreshold = 0.5\n((test_results>threshold)==test_labels).sum()/np.prod(test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T12:58:27.924249Z","iopub.execute_input":"2024-07-04T12:58:27.924630Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1859142592.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  t  = torch.Tensor(test_loader.dataset)\n","output_type":"stream"}]},{"cell_type":"code","source":"# from sklearn.metrics import RocCurveDisplay\n\n# RocCurveDisplay.from_predictions(\n#    test_labels.flatten(), test_results.flatten())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# t  = torch.Tensor(test_loader.dataset)\n\n# # splid in inputs and labels\n# test_inputs = t[:,:-1]#.to(torch.float32)\n# test_labels = t[:,-1, np.newaxis]#.to(torch.float32)\n\n# test_results = model(test_inputs).detach()\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download","metadata":{}},{"cell_type":"code","source":"\n\nbuildings = []\nsat_images = []\nbuilding_masks = []\n\nfor city in cities: \n    buildings.append(datahandler.get_buildings(city))\n    sat_images.append(datahandler.get_satellite_image(city))\n    building_masks.append(datahandler.get_building_mask(city))\n\n# Plot the expected results for the first city \ndatahandler.plot(city[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import data_preparation\n\nfor city in cities:\n    data_preparation.create_tensor(city)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download","metadata":{}},{"cell_type":"code","source":"# Download \n\nfor city in cities: \n    sat_image = datahandler.get_satellite_image(city)\n    mask = datahandler.get_building_mask(city)\n\n# Plot the expected results for the first city \ndatahandler.plot(city[0])","metadata":{},"execution_count":null,"outputs":[]}]}