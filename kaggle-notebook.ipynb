{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":8858480,"datasetId":5299351,"databundleVersionId":9018413}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install cloud-tpu-client https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.13-cp38-cp38m-linux_x86_64.whl\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyrosm openeo lightning rasterio plotly","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:36:33.467633Z","iopub.execute_input":"2024-07-05T07:36:33.468344Z","iopub.status.idle":"2024-07-05T07:42:17.674792Z","shell.execute_reply.started":"2024-07-05T07:36:33.468315Z","shell.execute_reply":"2024-07-05T07:42:17.673671Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting pyrosm\n  Downloading pyrosm-0.6.2.tar.gz (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting openeo\n  Downloading openeo-0.30.0-py3-none-any.whl.metadata (7.3 kB)\nCollecting lightning\n  Downloading lightning-2.3.2-py3-none-any.whl.metadata (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: rasterio in /opt/conda/lib/python3.10/site-packages (1.3.10)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.18.0)\nCollecting python-rapidjson (from pyrosm)\n  Downloading python_rapidjson-1.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\nRequirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.10/site-packages (from pyrosm) (69.0.3)\nRequirement already satisfied: geopandas>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from pyrosm) (0.14.4)\nCollecting shapely>=2.0.1 (from pyrosm)\n  Downloading shapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\nCollecting cykhash (from pyrosm)\n  Downloading cykhash-2.0.1.tar.gz (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting pyrobuf (from pyrosm)\n  Using cached pyrobuf-0.9.3-cp310-cp310-linux_x86_64.whl\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from openeo) (2.32.3)\nRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from openeo) (1.26.4)\nRequirement already satisfied: xarray>=0.12.3 in /opt/conda/lib/python3.10/site-packages (from openeo) (2024.5.0)\nRequirement already satisfied: pandas>0.20.0 in /opt/conda/lib/python3.10/site-packages (from openeo) (2.2.1)\nCollecting pystac>=1.5.0 (from openeo)\n  Downloading pystac-1.10.1-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: deprecated>=1.2.12 in /opt/conda/lib/python3.10/site-packages (from openeo) (1.2.14)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.3.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.2)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.4.0.post0)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.4)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.5)\nRequirement already satisfied: affine in /opt/conda/lib/python3.10/site-packages (from rasterio) (2.4.0)\nRequirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from rasterio) (23.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from rasterio) (2024.2.2)\nRequirement already satisfied: click>=4.0 in /opt/conda/lib/python3.10/site-packages (from rasterio) (8.1.7)\nRequirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.10/site-packages (from rasterio) (0.7.2)\nRequirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from rasterio) (1.4.7)\nRequirement already satisfied: click-plugins in /opt/conda/lib/python3.10/site-packages (from rasterio) (1.1.1)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.2.3)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.12->openeo) (1.14.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: fiona>=1.8.21 in /opt/conda/lib/python3.10/site-packages (from geopandas>=0.12.0->pyrosm) (1.9.6)\nRequirement already satisfied: pyproj>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from geopandas>=0.12.0->pyrosm) (3.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>0.20.0->openeo) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>0.20.0->openeo) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>0.20.0->openeo) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->openeo) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->openeo) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->openeo) (1.26.18)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (3.1.2)\nCollecting packaging<25.0,>=20.0 (from lightning)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: cython>=0.23 in /opt/conda/lib/python3.10/site-packages (from pyrobuf->pyrosm) (3.0.8)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas>=0.12.0->pyrosm) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.0.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=2.0.0->lightning) (1.3.0)\nDownloading openeo-0.30.0-py3-none-any.whl (260 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.2/260.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightning-2.3.2-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pystac-1.10.1-py3-none-any.whl (182 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.9/182.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_rapidjson-1.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyrosm, cykhash\n  Building wheel for pyrosm (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyrosm: filename=pyrosm-0.6.2-cp310-cp310-linux_x86_64.whl size=2927741 sha256=8a9abf8ed6ce4085a5349f290ddbb1cacae45ed64848c8b66e98617c492bb13f\n  Stored in directory: /root/.cache/pip/wheels/18/21/22/b07b96a708420e351c553188667cfd6ebc7e78a011a8708cf4\n  Building wheel for cykhash (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for cykhash: filename=cykhash-2.0.1-cp310-cp310-linux_x86_64.whl size=786067 sha256=ff67e5cbd224218f9e8ba25f738f12a798cf5781beb0e1f3d42b407d951922c6\n  Stored in directory: /root/.cache/pip/wheels/ed/de/7a/4386df8e70276a0d2ec5e990db76b6c89889dc41cd627e1c14\nSuccessfully built pyrosm cykhash\nInstalling collected packages: cykhash, shapely, python-rapidjson, packaging, pystac, pyrobuf, pyrosm, openeo, lightning\n  Attempting uninstall: shapely\n    Found existing installation: Shapely 1.8.5.post1\n    Uninstalling Shapely-1.8.5.post1:\n      Successfully uninstalled Shapely-1.8.5.post1\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cykhash-2.0.1 lightning-2.3.2 openeo-0.30.0 packaging-24.1 pyrobuf-0.9.3 pyrosm-0.6.2 pystac-1.10.1 python-rapidjson-1.18 shapely-2.0.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"import logging\nfrom pyrosm.data import sources\nimport numpy as np\n\ndef setup_logger(level: int = logging.INFO):\n    \"\"\"\n    Set up a logger for the pipeline. \n    \"\"\"\n    logger = logging.getLogger()\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s\"\n    )\n    \n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatter)\n    \n    file_handler = logging.FileHandler(\"main.log\")\n    file_handler.setFormatter(formatter)\n\n    logger.setLevel(level)\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n\n    return logger\n\n\ndef get_available_cities():\n    \"\"\"\n    Return all available cities from pyrosm \n    \"\"\"\n    return sources.cities.available\n\n\ndef stretch_hist(band):\n    \"\"\"\n    Apply histogram stretching\"\"\"\n    p2, p98 = np.percentile(band, (0.5, 99.5))\n    return np.clip((band - p2) * 255.0 / (p98 - p2), 0, 255).astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:42:17.676813Z","iopub.execute_input":"2024-07-05T07:42:17.677137Z","iopub.status.idle":"2024-07-05T07:42:18.700432Z","shell.execute_reply.started":"2024-07-05T07:42:17.677106Z","shell.execute_reply":"2024-07-05T07:42:18.699690Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# data_acquisition","metadata":{}},{"cell_type":"code","source":"#basics\nimport os\nimport time\nimport json\nimport pickle\nimport openeo\nimport numpy as np\n\n# geography\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.features import geometry_mask\n\n\n#download\nimport pyrosm as pyr\nfrom openeo.rest import OpenEoApiError\nfrom openeo.processes import ProcessBuilder, if_, is_nan\n\n\n\n# plotting \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\n\n\n\nclass DataHandler: \n    def __init__(self, logger, path_to_data_directory = \"data\"):\n        \"\"\"\n        Initialize the DataHandler class and define openeo params.\n        \"\"\"\n        self.logger = logger\n        self.openeo_temporal_extent = [\"2023-05-01\", \"2023-09-30\"]\n        self.openeo_bands = [\"B04\", \"B03\", \"B02\", \"B08\", \"B12\", \"B11\", \"SCL\"]\n        self.openeo_max_cloud_cover = 30\n        self.openeo_spatial_resolution = 10\n        self.openeo_connection = None\n        self.openeo_collections = None\n        self.openeo_jobs = None#\n        self.path_to_data_directory = path_to_data_directory\n        \n        \n        if not os.path.exists(self.path_to_data_directory):\n            os.makedirs(self.path_to_data_directory)\n            logger.info(\"Created data directory\")\n        else:\n            logger.info(\"Data directory already exists\")\n\n\n    def create_directory(self, city: str):\n        \"\"\"\n        Create a directory for each city.\n        \"\"\"\n        os.makedirs(os.path.join(self.path_to_data_directory, city), exist_ok=True)\n        self.logger.info(f\"{city}: Directory available\")\n\n\n    def get_buildings(self, city: str):\n        \"\"\"\n        Return buildings for a given city\n        \"\"\"\n        self.create_directory(city)\n        \n        # Check if local data for city is available\n        if \"buildings.geojson\" in os.listdir(os.path.join(self.path_to_data_directory, city)):\n            self.logger.info(f\"{city}: Using local building data\")\n            return gpd.read_file(os.path.join(self.path_to_data_directory, city,\"buildings.geojson\"))\n\n        # Download data for city\n        fp = pyr.get_data(city, directory=os.path.join(self.path_to_data_directory, city))\n        osm = pyr.OSM(fp)\n        self.logger.info(f\"{city}: Downloaded data to {self.path_to_data_directory}/{city}\")\n\n        # Get bounding box for city\n        boundingbox = self.get_boundingbox(city, osm)\n\n        # Get the buildings of the city\n        buildings_geodf = osm.get_buildings()\n\n        # Remove buildings outside of the bounding box of the city\n        buildings_geodf = buildings_geodf.cx[boundingbox[0] : boundingbox[2], boundingbox[1] : boundingbox[3]]\n\n        # Save the data of the city\n        buildings_path = os.path.join(self.path_to_data_directory, city,\"buildings.geojson\")\n        buildings_geodf.to_file(buildings_path, driver=\"GeoJSON\")\n        self.logger.info(f\"{city}: Stored data to {buildings_path}\")\n\n        return buildings_geodf\n\n\n    def get_boundingbox(self, city: str, osm = None):\n        \"\"\"\n        Get the bounding box for a city.\n        \"\"\"\n\n        # Return bounding box for Berlin as specified in exercise sheet to ensure correct testing results\n        if city == \"Berlin\":\n            return [13.294333, 52.454927, 13.500205, 52.574409]\n\n        # Check if local bounds are available\n        bounds_path = os.path.join(self.path_to_data_directory, city,\"bounds.pkl\")\n        if os.path.exists(bounds_path):\n            with open(bounds_path, \"rb\") as f:\n                boundingbox = pickle.load(f)\n            return boundingbox\n        \n        # Ensure OSM data is available \n        if osm is None:\n            self.get_buildings(city=city)\n\n        # Get the boundaries\n        geoframe_bounds = osm.get_boundaries()\n        boundingbox = geoframe_bounds[geoframe_bounds[\"name\"] == city].total_bounds\n\n        # Check if bounding box is None\n        if np.isnan(boundingbox[0]) or np.isnan(boundingbox[1]) or np.isnan(boundingbox[2]) or np.isnan(boundingbox[3]):\n            self.logger.info(f\"{city}: Bounding box is None. Using total bounds instead\")\n            boundingbox = geoframe_bounds.total_bounds\n        self.logger.info(f\"{city}: Bounding box is {boundingbox}\")     \n\n        # Save total bounds to pickle file\n        with open(bounds_path, \"wb\") as f:\n            pickle.dump(boundingbox, f)\n        self.logger.info(f\"{city}: Saved bounds to {bounds_path}\")\n\n        return boundingbox\n    \n\n    def get_satellite_image(self, city: str, return_rasterio_dataset = False): \n        \"\"\"\n        Get satellite images for a city. Use local data if available. Returns an Array with (H, W, C) shape\n        \"\"\"\n        if os.path.exists(os.path.join(self.path_to_data_directory, city,\"openEO.tif\")):\n            self.logger.info(f\"{city}: Using local satellite image\")\n            ds = rasterio.open(os.path.join(self.path_to_data_directory, city,\"openEO.tif\"))\n            if return_rasterio_dataset:\n                return ds\n            \n            # Read all channels\n            sat_data = ds.read()\n\n            # Transpose to (H, W, C)\n            sat_data = np.transpose(sat_data, (1, 2, 0))\n            return sat_data\n        else:\n            self.download_satellite_image(city)\n            return self.get_satellite_image(city)\n    \n\n    def connect_to_openeo(self):\n        \"\"\"\n        Connect to the openEO backend and \n        \"\"\"\n        if self.openeo_connection is None:\n            connection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\n            connection.authenticate_oidc()\n            self.openeo_connection = connection\n\n            self.logger.info(\"Connected to openEO\")\n        else:\n            self.logger.info(\"Already connected to openEO\")\n\n\n    def download_satellite_image(self, city: str):\n        \"\"\"\n        Download satellite images for a city. Retry for 3 times if the job fails or takes longer than 30 min per job.\n        \"\"\"\n        self.connect_to_openeo()\n        \n        # Log the currently running jobs\n        self.logger.info(\"Current jobs:\")\n        for idx, job in enumerate(self.openeo_connection.list_jobs()):\n            self.logger.info(f\"{idx} {job['id']} {job['status']}\")\n\n        # Retry job up to 3 times. Raise exception after 3 retries.\n        job_finished = False\n        job_number_of_retries = 0\n        while not job_finished : \n            if job_number_of_retries > 3:\n                self.logger.error(f\"{city}: Job failed after 3 retries\")\n                raise Exception(f\"{city}: Job failed after 3 retries\")\n            job = self.create_and_start_openeo_job(city)    \n            job_finished = self.await_job(city, job)\n            job_number_of_retries += 1\n\n        # Get job results and store in data/city\n        job_results = self.openeo_connection.job(job.job_id).get_results()\n        job_results.download_files(os.path.join(self.path_to_data_directory, city))\n        self.logger.info(f\"{city}: Downloaded job results to {os.path.join(self.path_to_data_directory, city)}\")\n\n\n    def delete_jobs(self):\n        \"\"\"\n        Delete all jobs on the openEO backend. Use only for debugging. \n        \"\"\"\n        self.connect_to_openeo()\n\n        for idx, job in enumerate(self.openeo_connection.list_jobs()):\n            self.logger.info(f\"Deleting job {idx}, {job['id']}, {job['status']}\")\n            self.openeo_connection.job(job[\"id\"]).delete_job()\n\n\n    def create_and_start_openeo_job(self, city: str, collection_id: str = \"SENTINEL2_L2A\"):\n        \"\"\"\n        Creates an openeo processing job for a city and starts it.\n        \"\"\"\n        # Transform order in boundingbox to dict\n        boundingbox = self.get_boundingbox(city)\n        boundingbox = {\"west\": boundingbox[0], \"south\": boundingbox[1], \"east\": boundingbox[2], \"north\": boundingbox[3]}\n        \n        # Create datacube\n        datacube = self.openeo_connection.load_collection(\n            collection_id=collection_id,\n            spatial_extent=boundingbox,\n            temporal_extent=self.openeo_temporal_extent,\n            bands=self.openeo_bands,\n            max_cloud_cover=self.openeo_max_cloud_cover,\n        ).resample_spatial(self.openeo_spatial_resolution)\n\n        # Create cloud mask\n        scl = datacube.band(\"SCL\")\n\n        # Filter out cloud median probability, cloud high probability, and snow/ice\n        mask = (scl == 8) | (scl == 9) | (scl == 11)\n\n        # Resample mask to the spatial resolution of the datacube\n        mask = mask.resample_cube_spatial(datacube.band(\"B04\"))\n        \n        # Create the RGB image\n        datacube_rgbFU = datacube.filter_bands(self.openeo_bands[:-1])\n        \n        # Apply cloud mask\n        datacube_rgb_masked = datacube_rgbFU.mask(mask)\n        \n        # Reduce temporal to median \n        datacube_rgb_masked_reduced_t = datacube_rgb_masked.reduce_temporal(\"median\")\n\n        # Define image format \n        datacube_for_submission = datacube_rgb_masked_reduced_t.save_result(format=\"GTiff\")\n        \n        # Create openEO job with datacube\n        job = datacube_for_submission.create_job(title=f\"{city}__pic\")\n        self.logger.info(f\"{city}: Created openEO job\")\n\n        # Start openEO job\n        job.start_job()\n        self.logger.info(f\"{city}: Started openEO job with ID: {job.job_id}\")        \n\n        return job\n\n\n    def await_job(self, city, job):\n        \"\"\"\n        Awaits the processing of a openeo job. \n        Returns when the job is finished or raises an exception if the job failed.\n        \"\"\"\n\n        for i in range(30):\n            status = self.openeo_connection.job(job.job_id).status()\n            self.logger.debug(f\"{city}: Job {job.job_id} status: {status}\")\n          \n            if status == \"finished\":\n                self.logger.info(f\"{city}: Job {job.job_id} finished\")\n                return True\n            \n            elif status == \"error\":\n                self.logger.warning(f\"{city}: Job {job.job_id} failed. Trying again.\")\n                return False            \n            \n            time.sleep(60)\n        self.logger.error(f\"{city}: Job {job.job_id} did not finish in time\")\n        return False\n\n    def get_building_mask(self, city: str, loaded_buildings = None, all_touched: bool = False):  \n        \"\"\"\n        Get the local building mask for buildings in a city.\n        \"\"\"\n        if all_touched:\n            filename = \"building_mask_dense\"\n        else:\n            filename = \"building_mask_sparse\"\n        # Check if the building mask is already available\n        if os.path.exists(os.path.join(self.path_to_data_directory, city,f\"{filename}.tif\")):\n            self.logger.info(f\"{city}: Using local building mask\")\n            return rasterio.open(os.path.join(self.path_to_data_directory, city,f\"{filename}.tif\")).read(1)\n\n        # Create new building mask \n        satellite_image = self.get_satellite_image(city, return_rasterio_dataset=True)\n\n        # Get satellite image metadata\n        transform = satellite_image.transform\n        out_shape = (satellite_image.height, satellite_image.width)\n        crs = satellite_image.crs\n\n        # Read the GeoJSON file with building polygons\n        if loaded_buildings is not None:\n            buildings = loaded_buildings\n        else:\n            buildings = self.get_buildings(city)\n            buildings = buildings.to_crs(crs)  # Ensure the CRS matches the GeoTIFF\n\n        # Create a mask where pixels inside buildings are True, others are False\n        # TODO all_touched paramer nutzen für zweite Maske\n        mask = geometry_mask(\n            buildings.geometry, transform=transform, invert=True, out_shape=out_shape, all_touched=all_touched,\n        )\n        \n        # Store the mask as a GeoTIFF file\n        \n        out_meta = satellite_image.meta\n        out_meta.update(\n            {\n                \"driver\": \"GTiff\",\n                \"height\": mask.shape[0],\n                \"width\": mask.shape[1],\n                # \"transform\": transform,\n                \"count\": 1,\n            }\n        )\n\n        # boolmask is automatically being saved as int16 [0,1]\n  \n        with rasterio.open(os.path.join(self.path_to_data_directory, city,f\"{filename}.tif\"), \"w\", **out_meta) as dest:\n            dest.write(mask, indexes=1)\n\n        return mask\n\n\n\n    def plot(self, city: str = \"BerlinTest\", \n\n             backend: str = \"matplotlib\",\n             figure_size: tuple = (10, 10),\n             brightness: int = 5,\n             image_directory: str = \"img/\",\n             show_plot: bool = False,\n             slice_to_be_plotted = None\n             ):\n        \"\"\"\n        Plot the data for a city either with matplotlib or plotly.\n        \"\"\"\n\n    \n    \n        if backend != \"plotly\" and backend != \"matplotlib\":            \n            raise NotImplementedError(\"Only matplotlib and plotly is supported at the moment\")\n        \n        satellite_data = self.get_satellite_image(city)        \n        mask = self.get_building_mask(city)\n        # Take out slice if only a slice is to be plotted\n        if slice_to_be_plotted is not None:\n            satellite_data = satellite_data[slice(*slice_to_be_plotted)]\n            mask = mask[slice(*slice_to_be_plotted)]\n        \n        if backend ==\"matplotlib\":\n            #load buildings\n            buildings = self.get_buildings(city)\n\n            # create image out path\n            image_path_out = os.path.join(image_directory, city)\n             # make the output directory if not exists\n            os.makedirs(image_path_out, exist_ok=True)\n\n            # Design plots\n            fig, ax = plt.subplots(figsize=figure_size)\n            buildings.plot(ax=ax, color=\"black\")\n            plt.title(f\"{city} buildings\")\n            plt.axis(\"off\")\n\n        # RGB Bands from Sentinel 2\n        red = satellite_data[...,0]\n        green = satellite_data[...,1]\n        blue = satellite_data[...,2]\n\n        # Apply histogram stretching\n        red_stretched = stretch_hist(red)\n        green_stretched = stretch_hist(green)\n        blue_stretched = stretch_hist(blue)\n\n        # Stack the bands after stretching\n        rgb_stretched = np.dstack((red_stretched, green_stretched, blue_stretched))\n\n        \n\n        if backend ==\"matplotlib\":\n            # Plot the histogram-stretched RGB image\n            plt.figure(figsize=figure_size)\n            plt.imshow(rgb_stretched)\n            # plt.title(\"Histogram Stretched RGB Composite Image\")\n            plt.title(f\"{city} RGB Bands from Sentinel-2 L2A\")\n            plt.axis(\"off\")\n            # plt.show()\n            plt.savefig(os.path.join(image_path_out, f\"{city}_RGB.png\"))\n            if show_plot:\n                plt.show()\n            plt.close()\n\n\n        # RGB image with higher brightness\n        red_norm = (red - np.min(red)) / (np.max(red) - np.min(red))\n        green_norm = (green - np.min(green)) / (np.max(green) - np.min(green))\n        blue_norm = (blue - np.min(blue)) / (np.max(blue) - np.min(blue))\n        pseudo_RGB_image = np.dstack((red_norm, green_norm, blue_norm))\n\n        pseudo_RGB_image_normalized = (pseudo_RGB_image - np.min(pseudo_RGB_image)) / (\n            pseudo_RGB_image.max() - pseudo_RGB_image.min()\n        )\n\n\n        pseudo_RGB_image_brighter = pseudo_RGB_image_normalized * brightness\n        pseudo_RGB_image_brighter = np.clip(pseudo_RGB_image_brighter, 0, 1)\n\n        if backend ==\"matplotlib\":\n            plt.figure(figsize=figure_size)\n            plt.imshow(pseudo_RGB_image_brighter)\n            plt.title(f\"{city} RGB Image\")\n            plt.axis(\"off\")\n            # plt.show()\n            plt.savefig(os.path.join(image_path_out, f\"{city}_RGB_Brighter.png\"))\n            if show_plot:\n                plt.show()\n            plt.close()\n\n            # single band img\n            # single_band = satellite_image.read(1)\n            single_band_stretched = stretch_hist(red)\n            plt.figure(figsize=figure_size)\n            plt.imshow(single_band_stretched, cmap=\"gray\")\n            plt.title(f\"{city} Single Band Image\")\n            plt.axis(\"off\")\n            # plt.show()\n            plt.savefig(os.path.join(image_path_out, f\"{city}_SingleBand.png\"))\n            if show_plot:\n                plt.show()\n            plt.close()\n        elif backend == \"plotly\":\n\n            # plot the mask\n            fig = px.imshow(mask.astype(np.uint8), binary_string=True)\n\n            # Overlay the mask with the image\n            fig.add_trace(go.Image(z=(pseudo_RGB_image_brighter * 255).astype(np.uint8), opacity=1))\n\n\n            # Update layout with a button to toggle mask visibility\n            fig.update_layout(\n                updatemenus=[\n                    dict(\n                        type=\"buttons\",\n                        direction=\"left\",\n                        buttons=list([\n                            dict(\n                                args=[{\"opacity\": [0,1]}],\n                                label=\"Hide Mask\",\n                                method=\"restyle\"\n                            ),\n                            dict(\n                                args=[{\"opacity\": [0.5, 0.5]}],\n                                label=\"Show Mask\",\n                                method=\"restyle\"\n                            )\n                        ]),\n                    ),\n                ],\n                xaxis=dict(\n                    scaleanchor=\"y\",\n                    scaleratio=1\n                ),\n                yaxis=dict(\n                    scaleanchor=\"x\",\n                    scaleratio=1\n                )\n            )\n\n            # Enable zooming and panning\n            fig.update_xaxes(constrain='domain')\n            fig.update_yaxes(scaleanchor='x', scaleratio=1)\n            fig.update_layout(height=1000, width=1000)\n\n            # Display the figure\n            return fig\n\n\n        # B8 B4 B3 -> False Color\n        b8 = satellite_data[...,3]\n        b8_stretched = stretch_hist(b8)\n        b4 = red_stretched\n        b3 = green_stretched\n\n        false_color = np.dstack((b8_stretched, b4, b3))\n        plt.figure(figsize=figure_size)\n        plt.imshow(false_color)\n        plt.title(f\"{city} False Color Image\")\n        plt.axis(\"off\")\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_FalseColor.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n        # params[\"bands\"] = [\"B04\", \"B03\", \"B02\", \"B08\", \"B12\", \"B11\", \"SCL\"] # scl must be last\n\n        # B12, B11, B4 -> False Color Urban\n        b12 = satellite_data[...,4]\n        b11 = satellite_data[...,5]\n        b04 = red\n        b12_norm = (b12 - np.min(b12)) / (np.max(b12) - np.min(b12))\n        b11_norm = (b11 - np.min(b11)) / (np.max(b11) - np.min(b11))\n        b04_norm = (b04 - np.min(b04)) / (np.max(b04) - np.min(b04))\n\n\n        false_color_urban = np.dstack((b12_norm, b11_norm, b04_norm)) * brightness\n        false_color_urban = np.clip(false_color_urban, 0, 1)\n\n        plt.figure(figsize=figure_size)\n        plt.imshow(false_color_urban)\n        plt.title(f\"{city} False Color Urban Image\")\n        plt.axis(\"off\")\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_FalseColorUrban.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n\n        # get vegetation_index\n        def vegetation_index(band1, band2):\n            return (band1 - band2) / (band1 + band2)\n\n\n        ndvi = vegetation_index(satellite_data[...,3], satellite_data[...,2])\n        plt.figure(figsize=figure_size)\n        plt.imshow(ndvi, cmap=\"RdYlGn\")\n        plt.title(f\"{city} NDVI Image\")\n        plt.axis(\"off\")\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_NDVI.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n        # Visualize the mask\n        plt.figure(figsize=(10, 10))\n        plt.imshow(mask, cmap=\"Blues\")\n        plt.title(f\"{city} Building Mask\")\n        plt.axis(\"off\")\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_BuildingMask.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n\n        # Load the image\n        img = single_band_stretched  # Assuming `blue_stretched` is the single band image\n        blue_cmap = plt.cm.Blues\n        blue_building_mask = blue_cmap(mask / mask.max())\n        blue_building_mask[..., 2] = mask * 0.8\n\n        # Plot the image\n        plt.figure(figsize=(10, 10))\n        plt.imshow(img, cmap=\"gray\", alpha=1)\n\n        plt.imshow(blue_building_mask)\n\n        # Set the title and axis labels\n        plt.title(f\"{city} Image with Buildings Mask\")\n        plt.axis(\"off\")\n\n        # Show the plot\n        # plt.show()\n        plt.savefig(os.path.join(image_path_out, f\"{city}_BuildingMaskOverlay.png\"))\n        if show_plot:\n            plt.show()\n        plt.close()\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:42:18.701922Z","iopub.execute_input":"2024-07-05T07:42:18.702345Z","iopub.status.idle":"2024-07-05T07:42:20.144491Z","shell.execute_reply.started":"2024-07-05T07:42:18.702320Z","shell.execute_reply":"2024-07-05T07:42:20.143744Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# data_preperation","metadata":{}},{"cell_type":"code","source":"import rasterio\nimport numpy as np\nimport torch\nfrom torch.utils.data import random_split, DataLoader, Dataset\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import kl_div\nimport pandas as pd\nimport logging\n\ndef create_tensor_of_windows(image, mask, patch_size=None):\n    \"\"\"\n    Create tensor with dimensions [N, H, W, C+1] from the satellite image of the city.\n    image should be of shape (H, W, C)\n    mask should be of shape (H, W, 1)\n    \"\"\"\n\n    # Merge Mask onto Image\n    image_with_mask = np.dstack((image, mask))\n\n    # cut of edges so image shape is divisible by patch size\n    reduced_image = image_with_mask[:-(image_with_mask.shape[0]%patch_size), :-(image_with_mask.shape[1]%patch_size)]\n\n    # get reduced image size\n    orig_img_x_size = reduced_image.shape[0]\n    orig_img_y_size = reduced_image.shape[1]\n\n    # get factors to which each dimension is reduced\n    down_scale_x = orig_img_x_size//patch_size\n    down_scale_y = orig_img_y_size//patch_size\n\n    # create array for pathched images\n    patched_images = np.zeros([down_scale_x*down_scale_y, patch_size, patch_size, reduced_image.shape[2]], dtype=np.uint16)\n\n    # fill array with patches\n    for i in range(down_scale_x):\n        for j in range(down_scale_y):\n            patched_images[i*down_scale_x+j::down_scale_x*down_scale_y] = reduced_image[patch_size*i:patch_size*(i+1), patch_size*j:patch_size*(j+1)]\n\n    # return array\n    return patched_images\n\n   \ndef divide_into_test_training(data, test_ratio=0.2, validation_ratio=0, seed=42):\n    \"\"\"\n    Divide the data into test and training split with seed.\n    \"\"\"\n    \n    # Define the split rati\n    train_ratio = 1 - test_ratio - validation_ratio\n    if train_ratio < 0:\n        raise ValueError(\"The train ratio is negative. Please check the split ratios.\")\n\n    # # Calculate the sizes for training and test sets\n    # train_size = int(train_ratio * len(data))\n    # test_size = int(test_ratio * len(data))\n    # validation_size = int(validation_ratio * len(data))\n\n    # Split the dataset with seed\n    generator = torch.Generator().manual_seed(seed)\n    train_dataset, test_dataset, validation_dataset = random_split(data, [train_ratio, test_ratio, validation_ratio], generator=generator)\n\n    return train_dataset, test_dataset, validation_dataset\n\n\ndef validate_test_training_validation_split(train_dataset, test_dataset, validation_dataset, city_names=None):\n    \"\"\"\n    Validate the train to the test split and the train to the validation split.\n    \"\"\"\n    logger = logging.getLogger()\n    # take out dataset for better readabiltiy\n    dataset = train_dataset.dataset[:,:-2]\n\n    # calculate mean, std, min and max for each image\n    means = dataset.mean(axis=(2,3))\n    stds = dataset.std(axis=(2,3))\n    mins = dataset.min(axis=(2,3))\n    maxs = dataset.max(axis=(2,3))\n\n    # create dataframes for train, test and validation set\n    train_means = pd.DataFrame({\n        \"mean\":means[train_dataset.indices].mean(axis=0), \n        \"std\":stds[train_dataset.indices].mean(axis=0),\n        \"min\":mins[train_dataset.indices].mean(axis=0),\n        \"max\":maxs[train_dataset.indices].mean(axis=0),\n        }, index=[\"R\", \"G\", \"B\",\"B08\", \"B12\", \"B11\"])\n\n    test_means = pd.DataFrame({\n        \"mean\":means[test_dataset.indices].mean(axis=0), \n        \"std\":stds[test_dataset.indices].mean(axis=0),\n        \"min\":mins[test_dataset.indices].mean(axis=0),\n        \"max\":maxs[test_dataset.indices].mean(axis=0),\n        }, index=[\"R\", \"G\", \"B\",\"B08\", \"B12\", \"B11\"])\n\n    # test if differences between train and test set are below 10%\n    if (((train_means-test_means)/train_means)<0.1).all().all():\n        print(u'\\u2713',\"Differences of train and test set is below 10% on mean, std, min and max across all input bands\",)\n    else:\n        # if not show differences and give out Warning\n        temp_df =(train_means-test_means)/train_means\n        logger.warning(\"Differences of train and test set is above 10% on one of mean, std, min and max across all input bands. This might be too big of a difference between train and test set. Please choose another seed for splitting.\")\n        print(\"!!!There might be large diffferecenes between train and test set. Please choose another seed for splitting. For more detail see the differences below\",)\n        print(temp_df[temp_df>0.1].dropna(axis=1, how='all').dropna(axis=0, how='all'))\n        \n    if validation_dataset.indices:\n        validation_means = pd.DataFrame({\n            \"mean\":means[validation_dataset.indices].mean(axis=0), \n            \"std\":stds[validation_dataset.indices].mean(axis=0),\n            \"min\":mins[validation_dataset.indices].mean(axis=0),\n            \"max\":maxs[validation_dataset.indices].mean(axis=0),\n            }, index=[\"R\", \"G\", \"B\",\"B08\", \"B12\", \"B11\"])\n        \n        # test if differences between train and validation set are below 10%\n        if (((train_means-validation_means)/train_means)<0.1).all().all():\n            print(u'\\u2713',\"Differences of train and validation set is below 10% on mean, std, min and max across all input bands\",)\n        else:\n            # if not show differences and give out Warning\n            temp_df =(train_means-validation_means)/train_means\n            logger.warning(\"Differences of train and validation set is above 10% on one of mean, std, min and max across all input bands. This might be too big of a difference between train and test set. Please choose another seed for splitting.\")\n            print(\"!!!There might be large diffferecenes between train and validation set. Please choose another seed for splitting. For more detail see the differences below\",)\n            print(temp_df[temp_df>0.1].dropna(axis=1, how='all').dropna(axis=0, how='all'))\n\n    print()\n    # Look at distribution of masks\n    masks  = train_dataset.dataset[:,[-2]]\n\n    # sum the pixels of building up over each image\n    sum = masks.sum(axis=(2,3))\n\n    # create dataframes for train, test and validation set with descriptive statistics\n    train_means_labels = pd.DataFrame({\n        \"mean\":sum[train_dataset.indices].mean(axis=0), \n        \"median\":np.median(sum[train_dataset.indices],axis=0), \n        \"std\":sum[train_dataset.indices].std(axis=0),\n        \"10th percentile\":np.percentile(sum[train_dataset.indices], q=10,axis=0),\n        \"90th percentile\":np.percentile(sum[train_dataset.indices], q=90,axis=0),\n        }, index=[\"Train\"])\n    test_means_labels = pd.DataFrame({\n        \"mean\":sum[test_dataset.indices].mean(axis=0),\n        \"median\":np.median(sum[test_dataset.indices],axis=0), \n        \"std\":sum[test_dataset.indices].std(axis=0),\n        \"10th percentile\":np.percentile(sum[test_dataset.indices], q=10,axis=0),\n        \"90th percentile\":np.percentile(sum[test_dataset.indices], q=90,axis=0),\n        }, index=[\"Test\"])\n    if validation_dataset.indices:\n        validation_means_labels = pd.DataFrame({\n            \"mean\":sum[validation_dataset.indices].mean(axis=0), \n            \"median\":np.median(sum[validation_dataset.indices],axis=0), \n            \"std\":sum[validation_dataset.indices].std(axis=0),\n            \"10th percentile\":np.percentile(sum[validation_dataset.indices], q=10,axis=0),\n            \"90th percentile\":np.percentile(sum[validation_dataset.indices], q=90,axis=0),\n            }, index=[\"Validation\"])\n        # print concatenated dataframes\n        print(\"Comparison of distribution of masks:\\n\",pd.concat([train_means_labels, test_means_labels, validation_means_labels]))\n    else:\n        print(\"Comparison of distribution of masks:\\n\", pd.concat([train_means_labels, test_means_labels] ))\n\n    print()\n\n    # look at the distribution of the data according to the different cities\n    if city_names is not None:  \n        # take out the city name for each label\n        train_cities = train_dataset.dataset[:,-1,0,0][train_dataset.indices]\n        test_cities = test_dataset.dataset[:,-1,0,0][test_dataset.indices]\n        validation_cities = validation_dataset.dataset[:,-1,0,0][validation_dataset.indices]\n\n        # lookup how often each city occured in the different sets\n        train_city_counts = np.unique(train_cities, return_counts=True)\n        test_city_counts = np.unique(test_cities, return_counts=True)\n        validation_citiy_counts = np.unique(validation_cities, return_counts=True)\n\n        # create dataframes for better readability\n        df = pd.DataFrame({\n            \"Train\":pd.Series(train_city_counts[1]/train_cities.shape[0], index=train_city_counts[0], name='train'),\n            \"Test\":pd.Series(test_city_counts[1]/test_cities.shape[0], index=test_city_counts[0], name='test'),\n            \"Validation\":pd.Series(validation_citiy_counts[1]/validation_cities.shape[0], index=validation_citiy_counts[0], name='validation')})\n        print(df.index)\n        df.index = df.index.map({i:c for i ,c in enumerate(city_names)})\n        print(\"Comparison of cities the data in the differen sets originates from:\\n\",df.T)\n\n\n\n\ndef create_data_loaders(train_dataset, test_dataset, validation_dataset, batch_size = 64):\n    \"\"\"\n    Create DataLoaders.\n    \"\"\"\n    # Create DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, test_loader, validation_loader\n\n\ndef apply_preprocessing_pipeline(images, masks, patch_size = 128, test_ratio = 0.2,validation_ratio=0, batch_size = 64, show_validation_of_split=True, city_names=None, minimum_number_of_true_pixels_per_image=0):\n    \"\"\"\n    applies windowing, deviding into train and test and creating data loaders.\n    \"\"\"\n\n    # for each city create patched images\n    patched_images = []\n    for i,(image, mask ) in enumerate(zip(images, masks)):\n        patched_image = create_tensor_of_windows(image, mask, patch_size=patch_size)\n        city = np.ones(shape=list(patched_image.shape[:-1])+[1])*i\n        patched_image_with_city = np.concatenate([patched_image, city], axis=-1)\n        patched_images.append(patched_image_with_city)\n\n\n    # concatenate all patched images\n    patched_images_merged = np.concatenate(patched_images, axis=0)\n\n    # reorder axis to [N, C, H, W] for torch\n    patched_images_merged = np.transpose(patched_images_merged, (0,3,1,2))\n    \n    # discard images with less than minimum_number_of_true_pixels_per_image\n    sums = patched_images_merged[:,-2].sum(axis=(1,2))\n    patched_images_merged = patched_images_merged[sums>=minimum_number_of_true_pixels_per_image]\n\n    # devide into train and test\n    train_dataset, test_dataset, validation_dataset = divide_into_test_training(patched_images_merged, test_ratio=test_ratio, validation_ratio=validation_ratio)\n\n    if show_validation_of_split:\n        validate_test_training_validation_split(train_dataset, test_dataset, validation_dataset, city_names=city_names)\n\n    dataset = train_dataset.dataset[:,:-1]\n    train_dataset.dataset = dataset\n    test_dataset.dataset = dataset\n    validation_dataset.dataset = dataset    \n    # create data loaders\n    train_loader, test_loader , validation_loader= create_data_loaders(train_dataset, test_dataset,validation_dataset, batch_size=batch_size)\n\n    # TODO fix error\n    # TODO remove city names\n\n    return train_loader, test_loader, validation_loader\n\n\ndef plot_sub_image( image_data):\n    \"\"\"\n    Plot sub image.\n    \"\"\"\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    ax[0].imshow(stretch_hist(image_data[:,:,:3]))\n    ax[1].imshow(stretch_hist(image_data[:,:,-1]))\n    return fig\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:42:20.146501Z","iopub.execute_input":"2024-07-05T07:42:20.147038Z","iopub.status.idle":"2024-07-05T07:42:22.102316Z","shell.execute_reply.started":"2024-07-05T07:42:20.147012Z","shell.execute_reply":"2024-07-05T07:42:22.101052Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"# basics\nimport os\n\nimport numpy as np\nfrom tqdm.notebook import tqdm \n\n\n# torch\nimport torch\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader\nimport lightning as L\n\n\n\n\n# custom modules\n\n\n\n\n# Configure logging for the pipeline\nlogger = setup_logger(level='ERROR')","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:42:22.103651Z","iopub.execute_input":"2024-07-05T07:42:22.104768Z","iopub.status.idle":"2024-07-05T07:42:36.533506Z","shell.execute_reply.started":"2024-07-05T07:42:22.104714Z","shell.execute_reply":"2024-07-05T07:42:36.532764Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-07-05 07:42:24.217182: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-05 07:42:24.217292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-05 07:42:24.363666: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"cities = ['London', 'CapeTown', 'Hamburg', 'Johannesburg', 'London', 'Montreal', 'Paris', 'Seoul', 'Singapore', 'Sydney']\n\ndatahandler = DataHandler(logger, '/kaggle/input/building-prediction/')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:42:36.534644Z","iopub.execute_input":"2024-07-05T07:42:36.535283Z","iopub.status.idle":"2024-07-05T07:42:36.541484Z","shell.execute_reply.started":"2024-07-05T07:42:36.535255Z","shell.execute_reply":"2024-07-05T07:42:36.540457Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# load images and mask for all specified cites\n\nimport os\nimages = []\nsparse_masks=[]\ndense_masks=[]\n\nfor city in cities:\n    buildings = None\n    if not os.path.exists(os.path.join(datahandler.path_to_data_directory,city,'building_mask_dense.tif')):\n        print(\"loading local buildings\")\n        buildings = datahandler.get_buildings(city)\n    images.append(datahandler.get_satellite_image(city))\n    sparse_masks.append(datahandler.get_building_mask(city, all_touched=False, loaded_buildings=buildings))\n    dense_masks.append(datahandler.get_building_mask(city, all_touched=True, loaded_buildings=buildings))","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:42:36.542804Z","iopub.execute_input":"2024-07-05T07:42:36.543140Z","iopub.status.idle":"2024-07-05T07:43:21.252783Z","shell.execute_reply.started":"2024-07-05T07:42:36.543112Z","shell.execute_reply":"2024-07-05T07:43:21.251963Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\nWarning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n","output_type":"stream"}]},{"cell_type":"code","source":"masks = [sparse_masks[i]+dense_masks[i]for i in range(len(sparse_masks))]","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:21.253851Z","iopub.execute_input":"2024-07-05T07:43:21.254108Z","iopub.status.idle":"2024-07-05T07:43:21.425547Z","shell.execute_reply.started":"2024-07-05T07:43:21.254086Z","shell.execute_reply":"2024-07-05T07:43:21.424782Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"patch_size = 128\ntest_ratio= 0.2\nvalidation_ratio=0.2\nbatch_size = 64\nshow_validation_of_split=False\ncity_names=cities\nminimum_number_of_true_pixels_per_image=1","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:21.426710Z","iopub.execute_input":"2024-07-05T07:43:21.427063Z","iopub.status.idle":"2024-07-05T07:43:21.432433Z","shell.execute_reply.started":"2024-07-05T07:43:21.427033Z","shell.execute_reply":"2024-07-05T07:43:21.431587Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# for each city create patched images\npatched_images = []\nfor i,(image, mask ) in enumerate(zip(images, masks)):\n    patched_image = create_tensor_of_windows(image, mask, patch_size=patch_size)\n    city = np.ones(shape=list(patched_image.shape[:-1])+[1])*i\n    patched_image_with_city = np.concatenate([patched_image, city], axis=-1)\n    patched_images.append(patched_image_with_city.astype(np.int16))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:21.435318Z","iopub.execute_input":"2024-07-05T07:43:21.435592Z","iopub.status.idle":"2024-07-05T07:43:37.676568Z","shell.execute_reply.started":"2024-07-05T07:43:21.435570Z","shell.execute_reply":"2024-07-05T07:43:37.675791Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n                          locals().items())), key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:37.677996Z","iopub.execute_input":"2024-07-05T07:43:37.678261Z","iopub.status.idle":"2024-07-05T07:43:37.686199Z","shell.execute_reply.started":"2024-07-05T07:43:37.678238Z","shell.execute_reply":"2024-07-05T07:43:37.685232Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"       patched_image_with_city:  1.0 GiB\n                 patched_image: 224.0 MiB\n                          city: 128.0 MiB\n                          mask: 33.2 MiB\n                           _i2: 40.7 KiB\n                           _i5: 40.7 KiB\n                           _i6: 11.4 KiB\n                   DataHandler:  1.2 KiB\n                           _i1:  1.1 KiB\n                OpenEoApiError:  1.0 KiB\n","output_type":"stream"}]},{"cell_type":"code","source":"sys.getsizeof(patched_images[0])/1e6","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:37.687566Z","iopub.execute_input":"2024-07-05T07:43:37.687901Z","iopub.status.idle":"2024-07-05T07:43:37.700222Z","shell.execute_reply.started":"2024-07-05T07:43:37.687877Z","shell.execute_reply":"2024-07-05T07:43:37.699457Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"557.842592"},"metadata":{}}]},{"cell_type":"code","source":"\n# concatenate all patched images\npatched_images_merged = np.concatenate(patched_images, axis=0)\n\n# reorder axis to [N, C, H, W] for torch\npatched_images_merged = np.transpose(patched_images_merged, (0,3,1,2))\n\n# discard images with less than minimum_number_of_true_pixels_per_image\nsums = patched_images_merged[:,-2].sum(axis=(1,2))\npatched_images_merged = patched_images_merged[sums>=minimum_number_of_true_pixels_per_image]\n\n# devide into train and test\ntrain_dataset, test_dataset, validation_dataset = divide_into_test_training(patched_images_merged, test_ratio=test_ratio, validation_ratio=validation_ratio)\n\nif show_validation_of_split:\n    validate_test_training_validation_split(train_dataset, test_dataset, validation_dataset, city_names=city_names)\n\ndataset = train_dataset.dataset[:,:-1]\ntrain_dataset.dataset = dataset\ntest_dataset.dataset = dataset\nvalidation_dataset.dataset = dataset    \n# create data loaders\ntrain_loader, test_loader , validation_loader= create_data_loaders(train_dataset, test_dataset,validation_dataset, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:37.701307Z","iopub.execute_input":"2024-07-05T07:43:37.701594Z","iopub.status.idle":"2024-07-05T07:43:40.022549Z","shell.execute_reply.started":"2024-07-05T07:43:37.701570Z","shell.execute_reply":"2024-07-05T07:43:40.021577Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# apply training pipeline\n# TODO make train test split consistent so we can train with multiple sizes, dont know if there is an advantage though\n# train_loader, test_loader , validation_loader= apply_preprocessing_pipeline(images, masks, patch_size = 128, test_ratio= 0.2,validation_ratio=0.2, batch_size = 64, show_validation_of_split=False,city_names=cities, minimum_number_of_true_pixels_per_image=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:40.023775Z","iopub.execute_input":"2024-07-05T07:43:40.024125Z","iopub.status.idle":"2024-07-05T07:43:40.028714Z","shell.execute_reply.started":"2024-07-05T07:43:40.024093Z","shell.execute_reply":"2024-07-05T07:43:40.027748Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class convNetSimple(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n                nn.Conv2d(6, 32, kernel_size=3, padding=1), nn.ReLU(),\n                nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n                nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n                nn.Conv2d(128, 1, kernel_size=1, padding=0),\n        )#nn.Sigmoid())\n    \n    def forward(self, x):\n        return self.model(x)\n    \nclass LitNet(L.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.loss = nn.MSELoss()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch[:,:-1], batch[:,-1]\n        outs = self.model(x.float())\n        loss = self.loss(outs, y.unsqueeze(1).float())\n        self.log(\"train_loss\", value=loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch[:,:-1], batch[:,-1]\n        outs = self.model(x.float())\n        loss = self.loss(outs, y.unsqueeze(1).float())\n        \n        values = {\n            \"test_loss\": loss,\n        }\n        self.log_dict(values, on_epoch=True, on_step=True, prog_bar=True, logger=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n    \n    def forward(self, x):\n        return self.model(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:40.029823Z","iopub.execute_input":"2024-07-05T07:43:40.030066Z","iopub.status.idle":"2024-07-05T07:43:40.042726Z","shell.execute_reply.started":"2024-07-05T07:43:40.030045Z","shell.execute_reply":"2024-07-05T07:43:40.041863Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Load Model","metadata":{}},{"cell_type":"code","source":"# load trained model\n# convmodel = LitNet(convNetSimple())\n# model_dict = torch.load(\"models/lightning_logs/version_1/checkpoints/epoch=39-step=7000.ckpt\", map_location=torch.device('cpu'))\n# convmodel.load_state_dict(model_dict['state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:40.043913Z","iopub.execute_input":"2024-07-05T07:43:40.044231Z","iopub.status.idle":"2024-07-05T07:43:40.055014Z","shell.execute_reply.started":"2024-07-05T07:43:40.044202Z","shell.execute_reply":"2024-07-05T07:43:40.054194Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n\nimport pytorch_lightning as pl\n\nL.seed_everything(42)\nconvmodel = LitNet(convNetSimple())\ntrainer = L.Trainer(\n    default_root_dir=\"models\",\n    # callbacks=[\n    #     EarlyStopping(\n    #         monitor=\"val_loss\",\n    #         mode=\"min\",\n    #         patience=10,\n    #     )\n    #     ModelCheckpoint(\n    #         monitor=\"val_loss\",\n    #         mode=\"min\",\n    #         save_top_k=2,\n    #         dirpath=\"models\",\n    #         filename=\"best_model\"\n    #     )\n    # ]\n    # val_check_interval=1,\n    fast_dev_run=False,\n    # num_sanity_val_steps=2,\n    max_epochs=300,\n    log_every_n_steps=20,\n    accelerator=\"gpu\", devices=2, strategy=\"ddp_notebook\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:40.056079Z","iopub.execute_input":"2024-07-05T07:43:40.056388Z","iopub.status.idle":"2024-07-05T07:43:40.717163Z","shell.execute_reply.started":"2024-07-05T07:43:40.056359Z","shell.execute_reply":"2024-07-05T07:43:40.716461Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n2024-07-05 07:43:40,318 - lightning.fabric.utilities.seed - INFO - seed_everything - Seed set to 42\n2024-07-05 07:43:40,603 - pytorch_lightning.utilities.rank_zero - INFO - _info - GPU available: True (cuda), used: True\n2024-07-05 07:43:40,694 - pytorch_lightning.utilities.rank_zero - INFO - _info - TPU available: False, using: 0 TPU cores\n2024-07-05 07:43:40,695 - pytorch_lightning.utilities.rank_zero - INFO - _info - HPU available: False, using: 0 HPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader.dataset.dataset.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-04T19:29:26.498608Z","iopub.execute_input":"2024-07-04T19:29:26.498845Z","iopub.status.idle":"2024-07-04T19:29:26.504768Z","shell.execute_reply.started":"2024-07-04T19:29:26.498823Z","shell.execute_reply":"2024-07-04T19:29:26.503936Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(8762, 7, 128, 128)"},"metadata":{}}]},{"cell_type":"code","source":"\n# training\ntrainer.fit(convmodel, \n    train_dataloaders=train_loader,\n    val_dataloaders=validation_loader\n)\n\n\n# testing\n\n# hier könnte man noch das beste model laden, wenn wir ein Val dataset haben.\n# best_model = LitModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n# trainer.test(\n#     best_model,\n#     dataloaders=test_loader\n# )\n\n\ntrainer.test(\n    convmodel,\n    dataloaders=test_loader\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T07:43:40.718093Z","iopub.execute_input":"2024-07-05T07:43:40.718342Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\nINFO: [rank: 0] Seed set to 42\n2024-07-05 07:43:40,866 - lightning.fabric.utilities.seed - INFO - seed_everything - [rank: 0] Seed set to 42\nINFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n2024-07-05 07:43:40,869 - lightning.fabric.utilities.distributed - INFO - _init_dist_connection - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\nINFO: [rank: 1] Seed set to 42\n2024-07-05 07:43:41,034 - lightning.fabric.utilities.seed - INFO - seed_everything - [rank: 1] Seed set to 42\nINFO: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n2024-07-05 07:43:41,038 - lightning.fabric.utilities.distributed - INFO - _init_dist_connection - Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\nWARNING: Missing logger folder: models/lightning_logs\n2024-07-05 07:43:41,044 - lightning.pytorch.loggers.tensorboard - WARNING - _get_next_version - Missing logger folder: models/lightning_logs\n2024-07-05 07:43:41,051 - pytorch_lightning.utilities.rank_zero - INFO - _info - ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\nWARNING: Missing logger folder: models/lightning_logs\n2024-07-05 07:43:41,053 - lightning.pytorch.loggers.tensorboard - WARNING - _get_next_version - Missing logger folder: models/lightning_logs\nINFO: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n2024-07-05 07:43:42,095 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n2024-07-05 07:43:42,095 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name  | Type          | Params | Mode \n------------------------------------------------\n0 | model | convNetSimple | 94.2 K | train\n1 | loss  | MSELoss       | 0      | train\n------------------------------------------------\n94.2 K    Trainable params\n0         Non-trainable params\n94.2 K    Total params\n0.377     Total estimated model params size (MB)\n2024-07-05 07:43:42,140 - lightning.pytorch.callbacks.model_summary - INFO - summarize - \n  | Name  | Type          | Params | Mode \n------------------------------------------------\n0 | model | convNetSimple | 94.2 K | train\n1 | loss  | MSELoss       | 0      | train\n------------------------------------------------\n94.2 K    Trainable params\n0         Non-trainable params\n94.2 K    Total params\n0.377     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18603dbf24214524b47a5690ebe62a18"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Instantiate the model, loss function, and optimizer\n# criterion = nn.BCELoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# # Training loop\n# num_epochs = 50\n\n# model.train()\n# for epoch in tqdm(range(num_epochs)):\n#     for batch in train_loader:\n#         # splid in inputs and labels\n#         inputs = batch[:,:-1].to(torch.float32)\n#         labels = batch[:,-1, np.newaxis].to(torch.float32)\n\n#         # zero the parameter gradients\n#         optimizer.zero_grad()\n\n#         # forward pass\n#         outputs = model(inputs)\n\n#         # calculate loss\n#         loss = criterion(outputs, labels)\n\n#         # write to tensorboard\n#         writer.add_scalar(\"Loss/train\", loss, epoch)\n\n#         # backward pass\n#         loss.backward()\n\n#         # optimizer step\n#         optimizer.step()\n    \n# writer.flush()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Model","metadata":{}},{"cell_type":"code","source":"# import os\n\n# os.makedirs(\"saved_models\", exist_ok=True)\n# torch.save(model.state_dict(), \"saved_models/model1\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"convmodel(inputs).detach().shape","metadata":{"execution":{"iopub.status.busy":"2024-07-04T13:48:36.472733Z","iopub.execute_input":"2024-07-04T13:48:36.473640Z","iopub.status.idle":"2024-07-04T13:48:39.494302Z","shell.execute_reply.started":"2024-07-04T13:48:36.473605Z","shell.execute_reply":"2024-07-04T13:48:39.493271Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"torch.Size([64, 1, 128, 128])"},"metadata":{}}]},{"cell_type":"code","source":"prediction = np.ones((len(test_loader.dataset.indices), 1,128, 128))*-1\ntrue_values = np.ones((len(test_loader.dataset.indices), 1, 128,128))*-1\nfor i,batch in enumerate(test_loader):\n    inputs = batch[:,:-1].to(torch.float32)\n    labels = batch[:,-1, np.newaxis].to(torch.float32)\n    prediction[i*64:(i+1)*64]=convmodel(inputs).detach()\n    true_values[i*64:(i+1)*64]=labels","metadata":{"execution":{"iopub.status.busy":"2024-07-04T13:49:24.569609Z","iopub.execute_input":"2024-07-04T13:49:24.570537Z","iopub.status.idle":"2024-07-04T13:50:06.109456Z","shell.execute_reply.started":"2024-07-04T13:49:24.570499Z","shell.execute_reply":"2024-07-04T13:50:06.108534Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"true_values.sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T13:50:40.059461Z","iopub.execute_input":"2024-07-04T13:50:40.061087Z","iopub.status.idle":"2024-07-04T13:50:40.084566Z","shell.execute_reply.started":"2024-07-04T13:50:40.061034Z","shell.execute_reply":"2024-07-04T13:50:40.082730Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"1048021.0"},"metadata":{}}]},{"cell_type":"code","source":"# predict on test set\n\n\nt  = torch.Tensor(test_loader.dataset)\n\n# splid in inputs and labels\ntest_inputs = t[:,:-1]#.to(torch.float32)\ntest_labels = t[:,-1, np.newaxis]#.to(torch.float32)\n\ntest_results = convmodel(test_inputs).detach()\n\n# Look at sums, to check if model only predicts zeros\nprint(\"Sum of test results: \", test_results.sum())\nprint(\"But it should be closer to: \", test_labels.sum())\n\n\n# # see how many percnet where predicted right\nthreshold = 0.5\n((test_results>threshold)==test_labels).sum()/np.prod(test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T12:58:27.924249Z","iopub.execute_input":"2024-07-04T12:58:27.924630Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1859142592.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  t  = torch.Tensor(test_loader.dataset)\n","output_type":"stream"}]},{"cell_type":"code","source":"# from sklearn.metrics import RocCurveDisplay\n\n# RocCurveDisplay.from_predictions(\n#    test_labels.flatten(), test_results.flatten())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# t  = torch.Tensor(test_loader.dataset)\n\n# # splid in inputs and labels\n# test_inputs = t[:,:-1]#.to(torch.float32)\n# test_labels = t[:,-1, np.newaxis]#.to(torch.float32)\n\n# test_results = model(test_inputs).detach()\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download","metadata":{}},{"cell_type":"code","source":"\n\nbuildings = []\nsat_images = []\nbuilding_masks = []\n\nfor city in cities: \n    buildings.append(datahandler.get_buildings(city))\n    sat_images.append(datahandler.get_satellite_image(city))\n    building_masks.append(datahandler.get_building_mask(city))\n\n# Plot the expected results for the first city \ndatahandler.plot(city[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import data_preparation\n\nfor city in cities:\n    data_preparation.create_tensor(city)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download","metadata":{}},{"cell_type":"code","source":"# Download \n\nfor city in cities: \n    sat_image = datahandler.get_satellite_image(city)\n    mask = datahandler.get_building_mask(city)\n\n# Plot the expected results for the first city \ndatahandler.plot(city[0])","metadata":{},"execution_count":null,"outputs":[]}]}