{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyrosm openeo lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919a90c",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d595d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyrosm.data import sources\n",
    "import numpy as np\n",
    "\n",
    "def setup_logger(level: int = logging.INFO):\n",
    "    \"\"\"\n",
    "    Set up a logger for the pipeline. \n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s\"\n",
    "    )\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    file_handler = logging.FileHandler(\"main.log\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_available_cities():\n",
    "    \"\"\"\n",
    "    Return all available cities from pyrosm \n",
    "    \"\"\"\n",
    "    return sources.cities.available\n",
    "\n",
    "\n",
    "def stretch_hist(band):\n",
    "    \"\"\"\n",
    "    Apply histogram stretching\"\"\"\n",
    "    p2, p98 = np.percentile(band, (0.5, 99.5))\n",
    "    return np.clip((band - p2) * 255.0 / (p98 - p2), 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c13b1",
   "metadata": {},
   "source": [
    "# data_acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babe46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import openeo\n",
    "import numpy as np\n",
    "\n",
    "# geography\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "\n",
    "#download\n",
    "import pyrosm as pyr\n",
    "from openeo.rest import OpenEoApiError\n",
    "from openeo.processes import ProcessBuilder, if_, is_nan\n",
    "\n",
    "\n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataHandler: \n",
    "    def __init__(self, logger, path_to_data_directory = \"data\"):\n",
    "        \"\"\"\n",
    "        Initialize the DataHandler class and define openeo params.\n",
    "        \"\"\"\n",
    "        self.logger = logger\n",
    "        self.openeo_temporal_extent = [\"2023-05-01\", \"2023-09-30\"]\n",
    "        self.openeo_bands = [\"B04\", \"B03\", \"B02\", \"B08\", \"B12\", \"B11\", \"SCL\"]\n",
    "        self.openeo_max_cloud_cover = 30\n",
    "        self.openeo_spatial_resolution = 10\n",
    "        self.openeo_connection = None\n",
    "        self.openeo_collections = None\n",
    "        self.openeo_jobs = None#\n",
    "        self.path_to_data_directory = path_to_data_directory\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(self.path_to_data_directory):\n",
    "            os.makedirs(self.path_to_data_directory)\n",
    "            logger.info(\"Created data directory\")\n",
    "        else:\n",
    "            logger.info(\"Data directory already exists\")\n",
    "\n",
    "\n",
    "    def create_directory(self, city: str):\n",
    "        \"\"\"\n",
    "        Create a directory for each city.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.join(self.path_to_data_directory, city), exist_ok=True)\n",
    "        self.logger.info(f\"{city}: Directory available\")\n",
    "\n",
    "\n",
    "    def get_buildings(self, city: str):\n",
    "        \"\"\"\n",
    "        Return buildings for a given city\n",
    "        \"\"\"\n",
    "        self.create_directory(city)\n",
    "        \n",
    "        # Check if local data for city is available\n",
    "        if \"buildings.geojson\" in os.listdir(os.path.join(self.path_to_data_directory, city)):\n",
    "            self.logger.info(f\"{city}: Using local building data\")\n",
    "            return gpd.read_file(os.path.join(self.path_to_data_directory, city,\"buildings.geojson\"))\n",
    "\n",
    "        # Download data for city\n",
    "        fp = pyr.get_data(city, directory=os.path.join(self.path_to_data_directory, city))\n",
    "        osm = pyr.OSM(fp)\n",
    "        self.logger.info(f\"{city}: Downloaded data to {self.path_to_data_directory}/{city}\")\n",
    "\n",
    "        # Get bounding box for city\n",
    "        boundingbox = self.get_boundingbox(city, osm)\n",
    "\n",
    "        # Get the buildings of the city\n",
    "        buildings_geodf = osm.get_buildings()\n",
    "\n",
    "        # Remove buildings outside of the bounding box of the city\n",
    "        buildings_geodf = buildings_geodf.cx[boundingbox[0] : boundingbox[2], boundingbox[1] : boundingbox[3]]\n",
    "\n",
    "        # Save the data of the city\n",
    "        buildings_path = os.path.join(self.path_to_data_directory, city,\"buildings.geojson\")\n",
    "        buildings_geodf.to_file(buildings_path, driver=\"GeoJSON\")\n",
    "        self.logger.info(f\"{city}: Stored data to {buildings_path}\")\n",
    "\n",
    "        return buildings_geodf\n",
    "\n",
    "\n",
    "    def get_boundingbox(self, city: str, osm = None):\n",
    "        \"\"\"\n",
    "        Get the bounding box for a city.\n",
    "        \"\"\"\n",
    "\n",
    "        # Return bounding box for Berlin as specified in exercise sheet to ensure correct testing results\n",
    "        if city == \"Berlin\":\n",
    "            return [13.294333, 52.454927, 13.500205, 52.574409]\n",
    "\n",
    "        # Check if local bounds are available\n",
    "        bounds_path = os.path.join(self.path_to_data_directory, city,\"bounds.pkl\")\n",
    "        if os.path.exists(bounds_path):\n",
    "            with open(bounds_path, \"rb\") as f:\n",
    "                boundingbox = pickle.load(f)\n",
    "            return boundingbox\n",
    "        \n",
    "        # Ensure OSM data is available \n",
    "        if osm is None:\n",
    "            self.get_buildings(city=city)\n",
    "\n",
    "        # Get the boundaries\n",
    "        geoframe_bounds = osm.get_boundaries()\n",
    "        boundingbox = geoframe_bounds[geoframe_bounds[\"name\"] == city].total_bounds\n",
    "\n",
    "        # Check if bounding box is None\n",
    "        if np.isnan(boundingbox[0]) or np.isnan(boundingbox[1]) or np.isnan(boundingbox[2]) or np.isnan(boundingbox[3]):\n",
    "            self.logger.info(f\"{city}: Bounding box is None. Using total bounds instead\")\n",
    "            boundingbox = geoframe_bounds.total_bounds\n",
    "        self.logger.info(f\"{city}: Bounding box is {boundingbox}\")     \n",
    "\n",
    "        # Save total bounds to pickle file\n",
    "        with open(bounds_path, \"wb\") as f:\n",
    "            pickle.dump(boundingbox, f)\n",
    "        self.logger.info(f\"{city}: Saved bounds to {bounds_path}\")\n",
    "\n",
    "        return boundingbox\n",
    "    \n",
    "\n",
    "    def get_satellite_image(self, city: str, return_rasterio_dataset = False): \n",
    "        \"\"\"\n",
    "        Get satellite images for a city. Use local data if available. Returns an Array with (H, W, C) shape\n",
    "        \"\"\"\n",
    "        if os.path.exists(os.path.join(self.path_to_data_directory, city,\"openEO.tif\")):\n",
    "            self.logger.info(f\"{city}: Using local satellite image\")\n",
    "            ds = rasterio.open(os.path.join(self.path_to_data_directory, city,\"openEO.tif\"))\n",
    "            if return_rasterio_dataset:\n",
    "                return ds\n",
    "            \n",
    "            # Read all channels\n",
    "            sat_data = ds.read()\n",
    "\n",
    "            # Transpose to (H, W, C)\n",
    "            sat_data = np.transpose(sat_data, (1, 2, 0))\n",
    "            return sat_data\n",
    "        else:\n",
    "            self.download_satellite_image(city)\n",
    "            return self.get_satellite_image(city)\n",
    "    \n",
    "\n",
    "    def connect_to_openeo(self):\n",
    "        \"\"\"\n",
    "        Connect to the openEO backend and \n",
    "        \"\"\"\n",
    "        if self.openeo_connection is None:\n",
    "            connection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\n",
    "            connection.authenticate_oidc()\n",
    "            self.openeo_connection = connection\n",
    "\n",
    "            self.logger.info(\"Connected to openEO\")\n",
    "        else:\n",
    "            self.logger.info(\"Already connected to openEO\")\n",
    "\n",
    "\n",
    "    def download_satellite_image(self, city: str):\n",
    "        \"\"\"\n",
    "        Download satellite images for a city. Retry for 3 times if the job fails or takes longer than 30 min per job.\n",
    "        \"\"\"\n",
    "        self.connect_to_openeo()\n",
    "        \n",
    "        # Log the currently running jobs\n",
    "        self.logger.info(\"Current jobs:\")\n",
    "        for idx, job in enumerate(self.openeo_connection.list_jobs()):\n",
    "            self.logger.info(f\"{idx} {job['id']} {job['status']}\")\n",
    "\n",
    "        # Retry job up to 3 times. Raise exception after 3 retries.\n",
    "        job_finished = False\n",
    "        job_number_of_retries = 0\n",
    "        while not job_finished : \n",
    "            if job_number_of_retries > 3:\n",
    "                self.logger.error(f\"{city}: Job failed after 3 retries\")\n",
    "                raise Exception(f\"{city}: Job failed after 3 retries\")\n",
    "            job = self.create_and_start_openeo_job(city)    \n",
    "            job_finished = self.await_job(city, job)\n",
    "            job_number_of_retries += 1\n",
    "\n",
    "        # Get job results and store in data/city\n",
    "        job_results = self.openeo_connection.job(job.job_id).get_results()\n",
    "        job_results.download_files(os.path.join(self.path_to_data_directory, city))\n",
    "        self.logger.info(f\"{city}: Downloaded job results to {os.path.join(self.path_to_data_directory, city)}\")\n",
    "\n",
    "\n",
    "    def delete_jobs(self):\n",
    "        \"\"\"\n",
    "        Delete all jobs on the openEO backend. Use only for debugging. \n",
    "        \"\"\"\n",
    "        self.connect_to_openeo()\n",
    "\n",
    "        for idx, job in enumerate(self.openeo_connection.list_jobs()):\n",
    "            self.logger.info(f\"Deleting job {idx}, {job['id']}, {job['status']}\")\n",
    "            self.openeo_connection.job(job[\"id\"]).delete_job()\n",
    "\n",
    "\n",
    "    def create_and_start_openeo_job(self, city: str, collection_id: str = \"SENTINEL2_L2A\"):\n",
    "        \"\"\"\n",
    "        Creates an openeo processing job for a city and starts it.\n",
    "        \"\"\"\n",
    "        # Transform order in boundingbox to dict\n",
    "        boundingbox = self.get_boundingbox(city)\n",
    "        boundingbox = {\"west\": boundingbox[0], \"south\": boundingbox[1], \"east\": boundingbox[2], \"north\": boundingbox[3]}\n",
    "        \n",
    "        # Create datacube\n",
    "        datacube = self.openeo_connection.load_collection(\n",
    "            collection_id=collection_id,\n",
    "            spatial_extent=boundingbox,\n",
    "            temporal_extent=self.openeo_temporal_extent,\n",
    "            bands=self.openeo_bands,\n",
    "            max_cloud_cover=self.openeo_max_cloud_cover,\n",
    "        ).resample_spatial(self.openeo_spatial_resolution)\n",
    "\n",
    "        # Create cloud mask\n",
    "        scl = datacube.band(\"SCL\")\n",
    "\n",
    "        # Filter out cloud median probability, cloud high probability, and snow/ice\n",
    "        mask = (scl == 8) | (scl == 9) | (scl == 11)\n",
    "\n",
    "        # Resample mask to the spatial resolution of the datacube\n",
    "        mask = mask.resample_cube_spatial(datacube.band(\"B04\"))\n",
    "        \n",
    "        # Create the RGB image\n",
    "        datacube_rgbFU = datacube.filter_bands(self.openeo_bands[:-1])\n",
    "        \n",
    "        # Apply cloud mask\n",
    "        datacube_rgb_masked = datacube_rgbFU.mask(mask)\n",
    "        \n",
    "        # Reduce temporal to median \n",
    "        datacube_rgb_masked_reduced_t = datacube_rgb_masked.reduce_temporal(\"median\")\n",
    "\n",
    "        # Define image format \n",
    "        datacube_for_submission = datacube_rgb_masked_reduced_t.save_result(format=\"GTiff\")\n",
    "        \n",
    "        # Create openEO job with datacube\n",
    "        job = datacube_for_submission.create_job(title=f\"{city}__pic\")\n",
    "        self.logger.info(f\"{city}: Created openEO job\")\n",
    "\n",
    "        # Start openEO job\n",
    "        job.start_job()\n",
    "        self.logger.info(f\"{city}: Started openEO job with ID: {job.job_id}\")        \n",
    "\n",
    "        return job\n",
    "\n",
    "\n",
    "    def await_job(self, city, job):\n",
    "        \"\"\"\n",
    "        Awaits the processing of a openeo job. \n",
    "        Returns when the job is finished or raises an exception if the job failed.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(30):\n",
    "            status = self.openeo_connection.job(job.job_id).status()\n",
    "            self.logger.debug(f\"{city}: Job {job.job_id} status: {status}\")\n",
    "          \n",
    "            if status == \"finished\":\n",
    "                self.logger.info(f\"{city}: Job {job.job_id} finished\")\n",
    "                return True\n",
    "            \n",
    "            elif status == \"error\":\n",
    "                self.logger.warning(f\"{city}: Job {job.job_id} failed. Trying again.\")\n",
    "                return False            \n",
    "            \n",
    "            time.sleep(60)\n",
    "        self.logger.error(f\"{city}: Job {job.job_id} did not finish in time\")\n",
    "        return False\n",
    "\n",
    "    def get_building_mask(self, city: str, loaded_buildings = None, all_touched: bool = False):  \n",
    "        \"\"\"\n",
    "        Get the local building mask for buildings in a city.\n",
    "        \"\"\"\n",
    "        if all_touched:\n",
    "            filename = \"building_mask_dense\"\n",
    "        else:\n",
    "            filename = \"building_mask_sparse\"\n",
    "        # Check if the building mask is already available\n",
    "        if os.path.exists(os.path.join(self.path_to_data_directory, city,f\"{filename}.tif\")):\n",
    "            self.logger.info(f\"{city}: Using local building mask\")\n",
    "            return rasterio.open(os.path.join(self.path_to_data_directory, city,f\"{filename}.tif\")).read(1)\n",
    "\n",
    "        # Create new building mask \n",
    "        satellite_image = self.get_satellite_image(city, return_rasterio_dataset=True)\n",
    "\n",
    "        # Get satellite image metadata\n",
    "        transform = satellite_image.transform\n",
    "        out_shape = (satellite_image.height, satellite_image.width)\n",
    "        crs = satellite_image.crs\n",
    "\n",
    "        # Read the GeoJSON file with building polygons\n",
    "        if loaded_buildings is not None:\n",
    "            buildings = loaded_buildings\n",
    "        else:\n",
    "            buildings = self.get_buildings(city)\n",
    "            buildings = buildings.to_crs(crs)  # Ensure the CRS matches the GeoTIFF\n",
    "\n",
    "        # Create a mask where pixels inside buildings are True, others are False\n",
    "        # TODO all_touched paramer nutzen für zweite Maske\n",
    "        mask = geometry_mask(\n",
    "            buildings.geometry, transform=transform, invert=True, out_shape=out_shape, all_touched=all_touched,\n",
    "        )\n",
    "        \n",
    "        # Store the mask as a GeoTIFF file\n",
    "        \n",
    "        out_meta = satellite_image.meta\n",
    "        out_meta.update(\n",
    "            {\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": mask.shape[0],\n",
    "                \"width\": mask.shape[1],\n",
    "                # \"transform\": transform,\n",
    "                \"count\": 1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # boolmask is automatically being saved as int16 [0,1]\n",
    "  \n",
    "        with rasterio.open(os.path.join(self.path_to_data_directory, city,f\"{filename}.tif\"), \"w\", **out_meta) as dest:\n",
    "            dest.write(mask, indexes=1)\n",
    "\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "    def plot(self, city: str = \"BerlinTest\", \n",
    "\n",
    "             backend: str = \"matplotlib\",\n",
    "             figure_size: tuple = (10, 10),\n",
    "             brightness: int = 5,\n",
    "             image_directory: str = \"img/\",\n",
    "             show_plot: bool = False,\n",
    "             slice_to_be_plotted = None\n",
    "             ):\n",
    "        \"\"\"\n",
    "        Plot the data for a city either with matplotlib or plotly.\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "        if backend != \"plotly\" and backend != \"matplotlib\":            \n",
    "            raise NotImplementedError(\"Only matplotlib and plotly is supported at the moment\")\n",
    "        \n",
    "        satellite_data = self.get_satellite_image(city)        \n",
    "        mask = self.get_building_mask(city)\n",
    "        # Take out slice if only a slice is to be plotted\n",
    "        if slice_to_be_plotted is not None:\n",
    "            satellite_data = satellite_data[slice(*slice_to_be_plotted)]\n",
    "            mask = mask[slice(*slice_to_be_plotted)]\n",
    "        \n",
    "        if backend ==\"matplotlib\":\n",
    "            #load buildings\n",
    "            buildings = self.get_buildings(city)\n",
    "\n",
    "            # create image out path\n",
    "            image_path_out = os.path.join(image_directory, city)\n",
    "             # make the output directory if not exists\n",
    "            os.makedirs(image_path_out, exist_ok=True)\n",
    "\n",
    "            # Design plots\n",
    "            fig, ax = plt.subplots(figsize=figure_size)\n",
    "            buildings.plot(ax=ax, color=\"black\")\n",
    "            plt.title(f\"{city} buildings\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        # RGB Bands from Sentinel 2\n",
    "        red = satellite_data[...,0]\n",
    "        green = satellite_data[...,1]\n",
    "        blue = satellite_data[...,2]\n",
    "\n",
    "        # Apply histogram stretching\n",
    "        red_stretched = stretch_hist(red)\n",
    "        green_stretched = stretch_hist(green)\n",
    "        blue_stretched = stretch_hist(blue)\n",
    "\n",
    "        # Stack the bands after stretching\n",
    "        rgb_stretched = np.dstack((red_stretched, green_stretched, blue_stretched))\n",
    "\n",
    "        \n",
    "\n",
    "        if backend ==\"matplotlib\":\n",
    "            # Plot the histogram-stretched RGB image\n",
    "            plt.figure(figsize=figure_size)\n",
    "            plt.imshow(rgb_stretched)\n",
    "            # plt.title(\"Histogram Stretched RGB Composite Image\")\n",
    "            plt.title(f\"{city} RGB Bands from Sentinel-2 L2A\")\n",
    "            plt.axis(\"off\")\n",
    "            # plt.show()\n",
    "            plt.savefig(os.path.join(image_path_out, f\"{city}_RGB.png\"))\n",
    "            if show_plot:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        # RGB image with higher brightness\n",
    "        red_norm = (red - np.min(red)) / (np.max(red) - np.min(red))\n",
    "        green_norm = (green - np.min(green)) / (np.max(green) - np.min(green))\n",
    "        blue_norm = (blue - np.min(blue)) / (np.max(blue) - np.min(blue))\n",
    "        pseudo_RGB_image = np.dstack((red_norm, green_norm, blue_norm))\n",
    "\n",
    "        pseudo_RGB_image_normalized = (pseudo_RGB_image - np.min(pseudo_RGB_image)) / (\n",
    "            pseudo_RGB_image.max() - pseudo_RGB_image.min()\n",
    "        )\n",
    "\n",
    "\n",
    "        pseudo_RGB_image_brighter = pseudo_RGB_image_normalized * brightness\n",
    "        pseudo_RGB_image_brighter = np.clip(pseudo_RGB_image_brighter, 0, 1)\n",
    "\n",
    "        if backend ==\"matplotlib\":\n",
    "            plt.figure(figsize=figure_size)\n",
    "            plt.imshow(pseudo_RGB_image_brighter)\n",
    "            plt.title(f\"{city} RGB Image\")\n",
    "            plt.axis(\"off\")\n",
    "            # plt.show()\n",
    "            plt.savefig(os.path.join(image_path_out, f\"{city}_RGB_Brighter.png\"))\n",
    "            if show_plot:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            # single band img\n",
    "            # single_band = satellite_image.read(1)\n",
    "            single_band_stretched = stretch_hist(red)\n",
    "            plt.figure(figsize=figure_size)\n",
    "            plt.imshow(single_band_stretched, cmap=\"gray\")\n",
    "            plt.title(f\"{city} Single Band Image\")\n",
    "            plt.axis(\"off\")\n",
    "            # plt.show()\n",
    "            plt.savefig(os.path.join(image_path_out, f\"{city}_SingleBand.png\"))\n",
    "            if show_plot:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "        elif backend == \"plotly\":\n",
    "\n",
    "            # plot the mask\n",
    "            fig = px.imshow(mask.astype(np.uint8), binary_string=True)\n",
    "\n",
    "            # Overlay the mask with the image\n",
    "            fig.add_trace(go.Image(z=(pseudo_RGB_image_brighter * 255).astype(np.uint8), opacity=1))\n",
    "\n",
    "\n",
    "            # Update layout with a button to toggle mask visibility\n",
    "            fig.update_layout(\n",
    "                updatemenus=[\n",
    "                    dict(\n",
    "                        type=\"buttons\",\n",
    "                        direction=\"left\",\n",
    "                        buttons=list([\n",
    "                            dict(\n",
    "                                args=[{\"opacity\": [0,1]}],\n",
    "                                label=\"Hide Mask\",\n",
    "                                method=\"restyle\"\n",
    "                            ),\n",
    "                            dict(\n",
    "                                args=[{\"opacity\": [0.5, 0.5]}],\n",
    "                                label=\"Show Mask\",\n",
    "                                method=\"restyle\"\n",
    "                            )\n",
    "                        ]),\n",
    "                    ),\n",
    "                ],\n",
    "                xaxis=dict(\n",
    "                    scaleanchor=\"y\",\n",
    "                    scaleratio=1\n",
    "                ),\n",
    "                yaxis=dict(\n",
    "                    scaleanchor=\"x\",\n",
    "                    scaleratio=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Enable zooming and panning\n",
    "            fig.update_xaxes(constrain='domain')\n",
    "            fig.update_yaxes(scaleanchor='x', scaleratio=1)\n",
    "            fig.update_layout(height=1000, width=1000)\n",
    "\n",
    "            # Display the figure\n",
    "            return fig\n",
    "\n",
    "\n",
    "        # B8 B4 B3 -> False Color\n",
    "        b8 = satellite_data[...,3]\n",
    "        b8_stretched = stretch_hist(b8)\n",
    "        b4 = red_stretched\n",
    "        b3 = green_stretched\n",
    "\n",
    "        false_color = np.dstack((b8_stretched, b4, b3))\n",
    "        plt.figure(figsize=figure_size)\n",
    "        plt.imshow(false_color)\n",
    "        plt.title(f\"{city} False Color Image\")\n",
    "        plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "        plt.savefig(os.path.join(image_path_out, f\"{city}_FalseColor.png\"))\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # params[\"bands\"] = [\"B04\", \"B03\", \"B02\", \"B08\", \"B12\", \"B11\", \"SCL\"] # scl must be last\n",
    "\n",
    "        # B12, B11, B4 -> False Color Urban\n",
    "        b12 = satellite_data[...,4]\n",
    "        b11 = satellite_data[...,5]\n",
    "        b04 = red\n",
    "        b12_norm = (b12 - np.min(b12)) / (np.max(b12) - np.min(b12))\n",
    "        b11_norm = (b11 - np.min(b11)) / (np.max(b11) - np.min(b11))\n",
    "        b04_norm = (b04 - np.min(b04)) / (np.max(b04) - np.min(b04))\n",
    "\n",
    "\n",
    "        false_color_urban = np.dstack((b12_norm, b11_norm, b04_norm)) * brightness\n",
    "        false_color_urban = np.clip(false_color_urban, 0, 1)\n",
    "\n",
    "        plt.figure(figsize=figure_size)\n",
    "        plt.imshow(false_color_urban)\n",
    "        plt.title(f\"{city} False Color Urban Image\")\n",
    "        plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "        plt.savefig(os.path.join(image_path_out, f\"{city}_FalseColorUrban.png\"))\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # get vegetation_index\n",
    "        def vegetation_index(band1, band2):\n",
    "            return (band1 - band2) / (band1 + band2)\n",
    "\n",
    "\n",
    "        ndvi = vegetation_index(satellite_data[...,3], satellite_data[...,2])\n",
    "        plt.figure(figsize=figure_size)\n",
    "        plt.imshow(ndvi, cmap=\"RdYlGn\")\n",
    "        plt.title(f\"{city} NDVI Image\")\n",
    "        plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "        plt.savefig(os.path.join(image_path_out, f\"{city}_NDVI.png\"))\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Visualize the mask\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(mask, cmap=\"Blues\")\n",
    "        plt.title(f\"{city} Building Mask\")\n",
    "        plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "        plt.savefig(os.path.join(image_path_out, f\"{city}_BuildingMask.png\"))\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # Load the image\n",
    "        img = single_band_stretched  # Assuming `blue_stretched` is the single band image\n",
    "        blue_cmap = plt.cm.Blues\n",
    "        blue_building_mask = blue_cmap(mask / mask.max())\n",
    "        blue_building_mask[..., 2] = mask * 0.8\n",
    "\n",
    "        # Plot the image\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(img, cmap=\"gray\", alpha=1)\n",
    "\n",
    "        plt.imshow(blue_building_mask)\n",
    "\n",
    "        # Set the title and axis labels\n",
    "        plt.title(f\"{city} Image with Buildings Mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Show the plot\n",
    "        # plt.show()\n",
    "        plt.savefig(os.path.join(image_path_out, f\"{city}_BuildingMaskOverlay.png\"))\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7babe",
   "metadata": {},
   "source": [
    "# data_preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import kl_div\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def create_tensor_of_windows(image, mask, patch_size=128):\n",
    "    \"\"\"\n",
    "    Create tensor with dimensions [N, H, W, C+1] from the satellite image of the city.\n",
    "    image should be of shape (H, W, C)\n",
    "    mask should be of shape (H, W, 1)\n",
    "    \"\"\"\n",
    "    # Merge Mask onto Image\n",
    "    image_with_mask = np.dstack((image, mask))\n",
    "\n",
    "    # cut of edges so image shape is divisible by patch size\n",
    "    reduced_image = image_with_mask[:-(image_with_mask.shape[0]%patch_size), :-(image_with_mask.shape[1]%patch_size)]\n",
    "\n",
    "    # calculate number of patches\n",
    "    N = reduced_image.shape[0]//patch_size*reduced_image.shape[1]//patch_size\n",
    "\n",
    "    # initialize target array\n",
    "    target_array = np.zeros((N, patch_size, patch_size, reduced_image.shape[-1]), dtype=np.uint16)\n",
    "\n",
    "    # fill target array\n",
    "    for row in range(patch_size):\n",
    "        for col in range(patch_size):\n",
    "            # calculate row and column indices\n",
    "            row_filter = range(row,reduced_image.shape[0]+row,patch_size)\n",
    "            col_filter = range(col,reduced_image.shape[1]+col,patch_size)\n",
    "\n",
    "            # write values into target array\n",
    "            target_array[:, row, col, :] = reduced_image[row_filter][:,col_filter,:].reshape(-1, reduced_image.shape[-1])\n",
    "\n",
    "    return target_array\n",
    "\n",
    "   \n",
    "def divide_into_test_training(data, test_ratio=0.2, validation_ratio=0, seed=42):\n",
    "    \"\"\"\n",
    "    Divide the data into test and training split with seed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the split rati\n",
    "    train_ratio = 1 - test_ratio - validation_ratio\n",
    "    if train_ratio < 0:\n",
    "        raise ValueError(\"The train ratio is negative. Please check the split ratios.\")\n",
    "\n",
    "    # # Calculate the sizes for training and test sets\n",
    "    # train_size = int(train_ratio * len(data))\n",
    "    # test_size = int(test_ratio * len(data))\n",
    "    # validation_size = int(validation_ratio * len(data))\n",
    "\n",
    "    # Split the dataset with seed\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, test_dataset, validation_dataset = random_split(data, [train_ratio, test_ratio, validation_ratio], generator=generator)\n",
    "\n",
    "    return train_dataset, test_dataset, validation_dataset\n",
    "\n",
    "\n",
    "def validate_test_training_validation_split(train_dataset, test_dataset, validation_dataset, city_names=None):\n",
    "    \"\"\"\n",
    "    Validate the train to the test split and the train to the validation split.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    # take out dataset for better readabiltiy\n",
    "    dataset = train_dataset.dataset[:,:-2]\n",
    "\n",
    "    # calculate mean, std, min and max for each image\n",
    "    means = dataset.mean(axis=(2,3))\n",
    "    stds = dataset.std(axis=(2,3))\n",
    "    mins = dataset.min(axis=(2,3))\n",
    "    maxs = dataset.max(axis=(2,3))\n",
    "\n",
    "    # create dataframes for train, test and validation set\n",
    "    train_means = pd.DataFrame({\n",
    "        \"mean\":means[train_dataset.indices].mean(axis=0), \n",
    "        \"std\":stds[train_dataset.indices].mean(axis=0),\n",
    "        \"min\":mins[train_dataset.indices].mean(axis=0),\n",
    "        \"max\":maxs[train_dataset.indices].mean(axis=0),\n",
    "        }, index=[\"R\", \"G\", \"B\",\"B08\", \"B12\", \"B11\"])\n",
    "\n",
    "    test_means = pd.DataFrame({\n",
    "        \"mean\":means[test_dataset.indices].mean(axis=0), \n",
    "        \"std\":stds[test_dataset.indices].mean(axis=0),\n",
    "        \"min\":mins[test_dataset.indices].mean(axis=0),\n",
    "        \"max\":maxs[test_dataset.indices].mean(axis=0),\n",
    "        }, index=[\"R\", \"G\", \"B\",\"B08\", \"B12\", \"B11\"])\n",
    "\n",
    "    # test if differences between train and test set are below 10%\n",
    "    if (((train_means-test_means)/train_means)<0.1).all().all():\n",
    "        print(u'\\u2713',\"Differences of train and test set is below 10% on mean, std, min and max across all input bands\",)\n",
    "    else:\n",
    "        # if not show differences and give out Warning\n",
    "        temp_df =(train_means-test_means)/train_means\n",
    "        logger.warning(\"Differences of train and test set is above 10% on one of mean, std, min and max across all input bands. This might be too big of a difference between train and test set. Please choose another seed for splitting.\")\n",
    "        print(\"!!!There might be large diffferecenes between train and test set. Please choose another seed for splitting. For more detail see the differences below\",)\n",
    "        print(temp_df[temp_df>0.1].dropna(axis=1, how='all').dropna(axis=0, how='all'))\n",
    "        \n",
    "    if validation_dataset.indices:\n",
    "        validation_means = pd.DataFrame({\n",
    "            \"mean\":means[validation_dataset.indices].mean(axis=0), \n",
    "            \"std\":stds[validation_dataset.indices].mean(axis=0),\n",
    "            \"min\":mins[validation_dataset.indices].mean(axis=0),\n",
    "            \"max\":maxs[validation_dataset.indices].mean(axis=0),\n",
    "            }, index=[\"R\", \"G\", \"B\",\"B08\", \"B12\", \"B11\"])\n",
    "        \n",
    "        # test if differences between train and validation set are below 10%\n",
    "        if (((train_means-validation_means)/train_means)<0.1).all().all():\n",
    "            print(u'\\u2713',\"Differences of train and validation set is below 10% on mean, std, min and max across all input bands\",)\n",
    "        else:\n",
    "            # if not show differences and give out Warning\n",
    "            temp_df =(train_means-validation_means)/train_means\n",
    "            logger.warning(\"Differences of train and validation set is above 10% on one of mean, std, min and max across all input bands. This might be too big of a difference between train and test set. Please choose another seed for splitting.\")\n",
    "            print(\"!!!There might be large diffferecenes between train and validation set. Please choose another seed for splitting. For more detail see the differences below\",)\n",
    "            print(temp_df[temp_df>0.1].dropna(axis=1, how='all').dropna(axis=0, how='all'))\n",
    "\n",
    "    print()\n",
    "    # Look at distribution of masks\n",
    "    masks  = train_dataset.dataset[:,[-2]]\n",
    "\n",
    "    # sum the pixels of building up over each image\n",
    "    sum = masks.sum(axis=(2,3))\n",
    "\n",
    "    # create dataframes for train, test and validation set with descriptive statistics\n",
    "    train_means_labels = pd.DataFrame({\n",
    "        \"mean\":sum[train_dataset.indices].mean(axis=0), \n",
    "        \"median\":np.median(sum[train_dataset.indices],axis=0), \n",
    "        \"std\":sum[train_dataset.indices].std(axis=0),\n",
    "        \"10th percentile\":np.percentile(sum[train_dataset.indices], q=10,axis=0),\n",
    "        \"90th percentile\":np.percentile(sum[train_dataset.indices], q=90,axis=0),\n",
    "        }, index=[\"Train\"])\n",
    "    test_means_labels = pd.DataFrame({\n",
    "        \"mean\":sum[test_dataset.indices].mean(axis=0),\n",
    "        \"median\":np.median(sum[test_dataset.indices],axis=0), \n",
    "        \"std\":sum[test_dataset.indices].std(axis=0),\n",
    "        \"10th percentile\":np.percentile(sum[test_dataset.indices], q=10,axis=0),\n",
    "        \"90th percentile\":np.percentile(sum[test_dataset.indices], q=90,axis=0),\n",
    "        }, index=[\"Test\"])\n",
    "    if validation_dataset.indices:\n",
    "        validation_means_labels = pd.DataFrame({\n",
    "            \"mean\":sum[validation_dataset.indices].mean(axis=0), \n",
    "            \"median\":np.median(sum[validation_dataset.indices],axis=0), \n",
    "            \"std\":sum[validation_dataset.indices].std(axis=0),\n",
    "            \"10th percentile\":np.percentile(sum[validation_dataset.indices], q=10,axis=0),\n",
    "            \"90th percentile\":np.percentile(sum[validation_dataset.indices], q=90,axis=0),\n",
    "            }, index=[\"Validation\"])\n",
    "        # print concatenated dataframes\n",
    "        print(\"Comparison of distribution of masks:\\n\",pd.concat([train_means_labels, test_means_labels, validation_means_labels]))\n",
    "    else:\n",
    "        print(\"Comparison of distribution of masks:\\n\", pd.concat([train_means_labels, test_means_labels] ))\n",
    "\n",
    "    print()\n",
    "\n",
    "    # look at the distribution of the data according to the different cities\n",
    "    if city_names is not None:  \n",
    "        # take out the city name for each label\n",
    "        train_cities = train_dataset.dataset[:,-1,0,0][train_dataset.indices]\n",
    "        test_cities = test_dataset.dataset[:,-1,0,0][test_dataset.indices]\n",
    "        validation_cities = validation_dataset.dataset[:,-1,0,0][validation_dataset.indices]\n",
    "\n",
    "        # lookup how often each city occured in the different sets\n",
    "        train_city_counts = np.unique(train_cities, return_counts=True)\n",
    "        test_city_counts = np.unique(test_cities, return_counts=True)\n",
    "        validation_citiy_counts = np.unique(validation_cities, return_counts=True)\n",
    "\n",
    "        # create dataframes for better readability\n",
    "        df = pd.DataFrame({\n",
    "            \"Train\":pd.Series(train_city_counts[1]/train_cities.shape[0], index=train_city_counts[0], name='train'),\n",
    "            \"Test\":pd.Series(test_city_counts[1]/test_cities.shape[0], index=test_city_counts[0], name='test'),\n",
    "            \"Validation\":pd.Series(validation_citiy_counts[1]/validation_cities.shape[0], index=validation_citiy_counts[0], name='validation')})\n",
    "        print(df.index)\n",
    "        df.index = df.index.map({i:c for i ,c in enumerate(city_names)})\n",
    "        print(\"Comparison of cities the data in the differen sets originates from:\\n\",df.T)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_data_loaders(train_dataset, test_dataset, validation_dataset, batch_size = 64):\n",
    "    \"\"\"\n",
    "    Create DataLoaders.\n",
    "    \"\"\"\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, validation_loader\n",
    "\n",
    "\n",
    "def apply_preprocessing_pipeline(images, masks, patch_size = 128, test_ratio = 0.2,validation_ratio=0, batch_size = 64, show_validation_of_split=True, city_names=None, minimum_number_of_true_pixels_per_image=0):\n",
    "    \"\"\"\n",
    "    applies windowing, deviding into train and test and creating data loaders.\n",
    "    \"\"\"\n",
    "\n",
    "    # for each city create patched images\n",
    "    patched_images = []\n",
    "    for i,(image, mask ) in enumerate(zip(images, masks)):\n",
    "        patched_image = create_tensor_of_windows(image, mask, patch_size=patch_size)\n",
    "        city = np.ones(shape=list(patched_image.shape[:-1])+[1])*i\n",
    "        patched_image_with_city = np.concatenate([patched_image, city], axis=-1)\n",
    "        patched_images.append(patched_image_with_city)\n",
    "\n",
    "\n",
    "    # concatenate all patched images\n",
    "    patched_images_merged = np.concatenate(patched_images, axis=0)\n",
    "\n",
    "    # reorder axis to [N, C, H, W] for torch\n",
    "    patched_images_merged = np.transpose(patched_images_merged, (0,3,1,2))\n",
    "    \n",
    "    # discard images with less than minimum_number_of_true_pixels_per_image\n",
    "    sums = patched_images_merged[:,-2].sum(axis=(1,2))\n",
    "    patched_images_merged = patched_images_merged[sums>=minimum_number_of_true_pixels_per_image]\n",
    "\n",
    "    # devide into train and test\n",
    "    train_dataset, test_dataset, validation_dataset = divide_into_test_training(patched_images_merged, test_ratio=test_ratio, validation_ratio=validation_ratio)\n",
    "\n",
    "    if show_validation_of_split:\n",
    "        validate_test_training_validation_split(train_dataset, test_dataset, validation_dataset, city_names=city_names)\n",
    "\n",
    "    dataset = train_dataset.dataset[:,:-1]\n",
    "    train_dataset.dataset = dataset\n",
    "    test_dataset.dataset = dataset\n",
    "    validation_dataset.dataset = dataset    \n",
    "    # create data loaders\n",
    "    train_loader, test_loader , validation_loader= create_data_loaders(train_dataset, test_dataset,validation_dataset, batch_size=batch_size)\n",
    "\n",
    "    # TODO fix error\n",
    "    # TODO remove city names\n",
    "\n",
    "    return train_loader, test_loader, validation_loader\n",
    "\n",
    "\n",
    "def plot_sub_image( image_data):\n",
    "    \"\"\"\n",
    "    Plot sub image.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    ax[0].imshow(stretch_hist(image_data[:,:,:3]))\n",
    "    ax[1].imshow(stretch_hist(image_data[:,:,-1]))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9685adf",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fe72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# custom modules\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging for the pipeline\n",
    "logger = setup_logger(level='ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd6248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['London', 'CapeTown', 'Hamburg', 'Johannesburg', 'London', 'Montreal', 'Paris', 'Seoul', 'Singapore', 'Sydney']\n",
    "\n",
    "datahandler = DataHandler(logger, '/kaggle/input/building-prediction/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becb350a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd18bbd3ce446b19097008ae1f4acbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
     ]
    }
   ],
   "source": [
    "# load images and mask for all specified cites\n",
    "\n",
    "import os\n",
    "images = []\n",
    "sparse_masks=[]\n",
    "dense_masks=[]\n",
    "\n",
    "for city in tqdm(cities):\n",
    "    buildings = None\n",
    "    if not os.path.exists(os.path.join(datahandler.path_to_data_directory,city,'building_mask_dense.tif')):\n",
    "        print(\"loading local buildings\")\n",
    "        buildings = datahandler.get_buildings(city)\n",
    "    images.append(datahandler.get_satellite_image(city))\n",
    "    sparse_masks.append(datahandler.get_building_mask(city, all_touched=False, loaded_buildings=buildings))\n",
    "    dense_masks.append(datahandler.get_building_mask(city, all_touched=True, loaded_buildings=buildings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90579204",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = sparse_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b029ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Differences of train and test set is below 10% on mean, std, min and max across all input bands\n",
      "!!!There might be large diffferecenes between train and validation set. Please choose another seed for splitting. For more detail see the differences below\n",
      "       std\n",
      "R  0.13538\n",
      "\n",
      "Comparison of distribution of masks:\n",
      "                   mean  median          std  10th percentile  90th percentile\n",
      "Train       877.862372   324.5  1186.567560             15.0           2599.0\n",
      "Test        815.344802   308.0  1126.038312             16.0           2411.5\n",
      "Validation  914.068864   413.0  1201.558744             18.0           2483.2\n",
      "\n",
      "Index([0.0, 1.0, 2.0, 3.0, 4.0], dtype='float64')\n",
      "Comparison of cities the data in the differen sets originates from:\n",
      "               London  CapeTown   Hamburg  Johannesburg    London\n",
      "Train       0.283065  0.090288  0.131772      0.206930  0.287945\n",
      "Test        0.285505  0.079795  0.125915      0.207174  0.301611\n",
      "Validation  0.310623  0.095238  0.131868      0.182418  0.279853\n"
     ]
    }
   ],
   "source": [
    "# apply training pipeline\n",
    "# TODO make train test split consistent so we can train with multiple sizes, dont know if there is an advantage though\n",
    "train_loader, test_loader , validation_loader= apply_preprocessing_pipeline(images, masks, patch_size = 128, test_ratio= 0.2,validation_ratio=0.2, batch_size = 64, show_validation_of_split=True,city_names=cities, minimum_number_of_true_pixels_per_image=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82934b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 7, 128, 128]), torch.Tensor, torch.uint16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_loader))   \n",
    "sample.shape, type(sample), sample.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70a71936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Conv2d(6, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "                nn.Conv2d(128, 1, kernel_size=1, padding=0),\n",
    "                nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class LitNet(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[:,:-1], batch[:,-1]\n",
    "        outs = self.model(x.float())\n",
    "        loss = self.loss(outs, y.unsqueeze(1).float())\n",
    "        self.log(\"train_loss\", value=loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch[:,:-1], batch[:,-1]\n",
    "        outs = self.model(x.float())\n",
    "        loss = self.loss(outs, y.unsqueeze(1).float())\n",
    "        \n",
    "        values = {\n",
    "            \"test_loss\": loss,\n",
    "        }\n",
    "        self.log_dict(values, on_epoch=True, on_step=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787baf8c",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da6a2a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model\n",
    "convmodel = LitNet(convNetSimple())\n",
    "model_dict = torch.load(\"models/lightning_logs/version_1/checkpoints/epoch=39-step=7000.ckpt\", map_location=torch.device('cpu'))\n",
    "convmodel.load_state_dict(model_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c2b6549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54b739d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n",
      "2024-06-28 23:29:49,796 - lightning.fabric.utilities.seed - INFO - seed_everything - Seed set to 42\n",
      "2024-06-28 23:29:49,796 - lightning.fabric.utilities.seed - INFO - seed_everything - Seed set to 42\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "2024-06-28 23:29:49,816 - lightning.pytorch.utilities.rank_zero - INFO - _info - GPU available: True (cuda), used: True\n",
      "2024-06-28 23:29:49,816 - lightning.pytorch.utilities.rank_zero - INFO - _info - GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "2024-06-28 23:29:49,818 - lightning.pytorch.utilities.rank_zero - INFO - _info - TPU available: False, using: 0 TPU cores\n",
      "2024-06-28 23:29:49,818 - lightning.pytorch.utilities.rank_zero - INFO - _info - TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "2024-06-28 23:29:49,819 - lightning.pytorch.utilities.rank_zero - INFO - _info - HPU available: False, using: 0 HPUs\n",
      "2024-06-28 23:29:49,819 - lightning.pytorch.utilities.rank_zero - INFO - _info - HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 23:29:49,823 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 23:29:49,823 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | convNetSimple | 94.2 K | train\n",
      "1 | loss  | BCELoss       | 0      | train\n",
      "------------------------------------------------\n",
      "94.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.2 K    Total params\n",
      "0.377     Total estimated model params size (MB)\n",
      "2024-06-28 23:29:49,836 - lightning.pytorch.callbacks.model_summary - INFO - summarize - \n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | convNetSimple | 94.2 K | train\n",
      "1 | loss  | BCELoss       | 0      | train\n",
      "------------------------------------------------\n",
      "94.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.2 K    Total params\n",
      "0.377     Total estimated model params size (MB)\n",
      "2024-06-28 23:29:49,836 - lightning.pytorch.callbacks.model_summary - INFO - summarize - \n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | convNetSimple | 94.2 K | train\n",
      "1 | loss  | BCELoss       | 0      | train\n",
      "------------------------------------------------\n",
      "94.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "94.2 K    Total params\n",
      "0.377     Total estimated model params size (MB)\n",
      "/home/jlb/Dev/architecture-of-ml-systems/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=20` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a2bdef495f4e469644770c2113461d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "2024-06-28 23:47:55,487 - lightning.pytorch.utilities.rank_zero - INFO - _info - `Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "2024-06-28 23:47:55,487 - lightning.pytorch.utilities.rank_zero - INFO - _info - `Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 23:47:55,519 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-06-28 23:47:55,519 - lightning.pytorch.accelerators.cuda - INFO - set_nvidia_flags - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jlb/Dev/architecture-of-ml-systems/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=20` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b6783770584ec982a5b3b9755c6e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_loss_epoch         9.542298316955566\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 9.542298316955566}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "\n",
    "L.seed_everything(42)\n",
    "convmodel = LitNet(convNetSimple())\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=\"models\",\n",
    "    # callbacks=[\n",
    "    #     EarlyStopping(\n",
    "    #         monitor=\"val_loss\",\n",
    "    #         mode=\"min\",\n",
    "    #         patience=10,\n",
    "    #     )\n",
    "    #     ModelCheckpoint(\n",
    "    #         monitor=\"val_loss\",\n",
    "    #         mode=\"min\",\n",
    "    #         save_top_k=2,\n",
    "    #         dirpath=\"models\",\n",
    "    #         filename=\"best_model\"\n",
    "    #     )\n",
    "    # ]\n",
    "    # val_check_interval=1,\n",
    "    fast_dev_run=False,\n",
    "    # num_sanity_val_steps=2,\n",
    "    max_epochs=40,\n",
    "    log_every_n_steps=20,\n",
    ")\n",
    "\n",
    "# training\n",
    "trainer.fit(convmodel, \n",
    "    train_dataloaders=train_loader,\n",
    "    # val_dataloaders=val_loader   #### HIER brauchen wir noch einen validation loader\n",
    ")\n",
    "\n",
    "\n",
    "# testing\n",
    "\n",
    "# hier könnte man noch das beste model laden, wenn wir ein Val dataset haben.\n",
    "# best_model = LitModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "# trainer.test(\n",
    "#     best_model,\n",
    "#     dataloaders=test_loader\n",
    "# )\n",
    "\n",
    "\n",
    "trainer.test(\n",
    "    convmodel,\n",
    "    dataloaders=test_loader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770044bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model, loss function, and optimizer\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 50\n",
    "\n",
    "# model.train()\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "#     for batch in train_loader:\n",
    "#         # splid in inputs and labels\n",
    "#         inputs = batch[:,:-1].to(torch.float32)\n",
    "#         labels = batch[:,-1, np.newaxis].to(torch.float32)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward pass\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # calculate loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # write to tensorboard\n",
    "#         writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "#         # backward pass\n",
    "#         loss.backward()\n",
    "\n",
    "#         # optimizer step\n",
    "#         optimizer.step()\n",
    "    \n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993a7a2",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bb75862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.makedirs(\"saved_models\", exist_ok=True)\n",
    "# torch.save(model.state_dict(), \"saved_models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74789b6d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "011a76ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9638)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on test set\n",
    "t  = torch.Tensor(test_loader.dataset)\n",
    "\n",
    "# splid in inputs and labels\n",
    "test_inputs = t[:,:-1]#.to(torch.float32)\n",
    "test_labels = t[:,-1, np.newaxis]#.to(torch.float32)\n",
    "\n",
    "test_results = convmodel(test_inputs).detach()\n",
    "\n",
    "# Look at sums, to check if model only predicts zeros\n",
    "print(\"Sum of test results: \", test_results.sum())\n",
    "print(\"But it should be closer to: \", test_labels.sum())\n",
    "\n",
    "\n",
    "# # see how many percnet where predicted right\n",
    "threshold = 0.5\n",
    "((test_results>threshold)==test_labels).sum()/np.prod(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495886ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# RocCurveDisplay.from_predictions(\n",
    "#    test_labels.flatten(), test_results.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dedb49a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a15a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t  = torch.Tensor(test_loader.dataset)\n",
    "\n",
    "# # splid in inputs and labels\n",
    "# test_inputs = t[:,:-1]#.to(torch.float32)\n",
    "# test_labels = t[:,-1, np.newaxis]#.to(torch.float32)\n",
    "\n",
    "# test_results = model(test_inputs).detach()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7153d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acca6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d532fb85",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "buildings = []\n",
    "sat_images = []\n",
    "building_masks = []\n",
    "\n",
    "for city in cities: \n",
    "    buildings.append(datahandler.get_buildings(city))\n",
    "    sat_images.append(datahandler.get_satellite_image(city))\n",
    "    building_masks.append(datahandler.get_building_mask(city))\n",
    "\n",
    "# Plot the expected results for the first city \n",
    "datahandler.plot(city[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preparation\n",
    "\n",
    "for city in cities:\n",
    "    data_preparation.create_tensor(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b5779",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download \n",
    "\n",
    "for city in cities: \n",
    "    sat_image = datahandler.get_satellite_image(city)\n",
    "    mask = datahandler.get_building_mask(city)\n",
    "\n",
    "# Plot the expected results for the first city \n",
    "datahandler.plot(city[0])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
