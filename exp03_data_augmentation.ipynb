{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "import utilities.utils as utils\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "\n",
    "from data_preperation.dataset import CityDataset\n",
    "import os\n",
    "\n",
    "from config import PATH, CITIES, MIN_LABELS, PATCH_SIZE, LOGLEVEL, TEST_CITY\n",
    "\n",
    "# custom modules\n",
    "from data_acquisition.datahandler import DataHandler\n",
    "\n",
    "\n",
    "# Configure logging for the pipeline\n",
    "logger = utils.setup_logger(level=LOGLEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import lightning as L\n",
    "from typing import Any\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from lightning import seed_everything\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from models.lightning_utils import LitModule\n",
    "from models.baseconvnet import ConvNetSimple\n",
    "\n",
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "\n",
    "# model\n",
    "# convmodel = LitModule(ConvNetSimple())\n",
    "\n",
    "\n",
    "# trainer\n",
    "def get_trainer(dirname):\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=f\"model_experiments/augmentation/{dirname}\",\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                mode=\"min\",\n",
    "                patience=5,\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                dirpath=f\"model_experiments/augmentation/{dirname}\",\n",
    "                monitor=\"val_loss\",\n",
    "                mode=\"min\",\n",
    "                save_top_k=1,\n",
    "                filename=\"best_model\",\n",
    "            ),\n",
    "        ],\n",
    "        # val_check_interval=1,\n",
    "        fast_dev_run=False,\n",
    "        num_sanity_val_steps=2,\n",
    "        max_epochs=100,\n",
    "        log_every_n_steps=20,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we import the Data Augmentation Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_augmentation.transformations import (\n",
    "    ColorJitterCustom,\n",
    "    random_rotation,\n",
    "    random_horizontal_flip,\n",
    "    random_vertical_flip,\n",
    ")\n",
    "\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    RandomChoice,\n",
    ")\n",
    "\n",
    "# transforms_train = Compose([\n",
    "#     random_rotation,\n",
    "#     random_horizontal_flip,\n",
    "#     random_vertical_flip,\n",
    "#     ColorJitterCustom(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
    "# ])\n",
    "\n",
    "\n",
    "# Compose the transformations\n",
    "custom_transforms1 = Compose(\n",
    "    [\n",
    "        random_rotation,\n",
    "        random_horizontal_flip,\n",
    "        random_vertical_flip,\n",
    "        ColorJitterCustom(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "custom_transforms2 = Compose(\n",
    "    [random_rotation, random_horizontal_flip, random_vertical_flip]\n",
    ")\n",
    "\n",
    "custom_transforms3 = Compose(\n",
    "    [\n",
    "        ColorJitterCustom(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def no_op(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "custom_transforms4 = Compose(\n",
    "    [\n",
    "        RandomChoice(\n",
    "            [random_rotation, random_horizontal_flip, random_vertical_flip, no_op]\n",
    "        ),\n",
    "        RandomChoice(\n",
    "            [random_rotation, random_horizontal_flip, random_vertical_flip, no_op]\n",
    "        ),\n",
    "        ColorJitterCustom(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transformations_list = [\n",
    "    custom_transforms1,\n",
    "    custom_transforms2,\n",
    "    custom_transforms3,\n",
    "    custom_transforms4,\n",
    "]\n",
    "# transformations_list = [custom_transforms3]\n",
    "# transformations_list = [custom_transforms4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")  # for tensor cores\n",
    "# from data_acquisition import DataHandler\n",
    "\n",
    "# datahandler = DataHandler(logger, path_to_data_directory=\"data\")\n",
    "\n",
    "\n",
    "# load images and mask for all specified cites\n",
    "\n",
    "# import os\n",
    "# images = []\n",
    "# dense_masks=[]\n",
    "# boundary_masks=[]\n",
    "\n",
    "# for city in tqdm(cities):\n",
    "#     buildings = None\n",
    "#     if not os.path.exists(os.path.join(datahandler.path_to_data_directory,city,'building_mask_dense.tif')):\n",
    "#         print(\"loading local buildings\")\n",
    "#         buildings = datahandler.get_buildings(city)\n",
    "#     images.append(datahandler.get_satellite_image(city).transpose(2, 0, 1))\n",
    "#     dense_masks.append(datahandler.get_building_mask(city, all_touched=True, loaded_buildings=buildings))\n",
    "#     boundary_masks.append(datahandler.get_boundaries_mask(city))\n",
    "\n",
    "# print(f\"Data len: {len(images)}, {len(dense_masks)}, {len(boundary_masks)}\")\n",
    "\n",
    "dataset = CityDataset(\n",
    "    PATH,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    data_name=\"openEO.tif\",\n",
    "    labels_name=\"building_mask_dense.tif\",\n",
    "    image_bands=[1, 2, 3, 4, 5, 6],\n",
    "    min_labels=MIN_LABELS,\n",
    "    # devrun=True,\n",
    "    cities=CITIES,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "train_ds, val_ds = dataset.train_val_split(val_size=0.1, show_summary=False)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=20)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=20)\n",
    "\n",
    "dataset_test = CityDataset(\n",
    "    PATH,\n",
    "    data_name=\"openEO.tif\",\n",
    "    labels_name=\"building_mask_dense.tif\",\n",
    "    image_bands=[1, 2, 3, 4, 5, 6],\n",
    "    # devrun=True,\n",
    "    cities=TEST_CITY,\n",
    "    train=False,\n",
    ")\n",
    "test_dl = DataLoader(dataset_test, batch_size=32, shuffle=False, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, transforms in enumerate(transformations_list):\n",
    "    idx = 4\n",
    "    # dataset.transform = transforms\n",
    "    dataset.transform = None\n",
    "    train_ds, val_ds = dataset.train_val_split(val_size=0.1, show_summary=False)\n",
    "    train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=20)\n",
    "    val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=20)\n",
    "    model = LitModule(ConvNetSimple(6), learning_rate=0.001, optimizer=\"adam\")\n",
    "    trainer = get_trainer(f\"convSimple/_{idx}_transformation\")\n",
    "    seed_everything(49)\n",
    "    trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "    best_model = LitModule.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path\n",
    "    )\n",
    "    trainer.test(model=model, dataloaders=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = trainer.predict(model=model, dataloaders=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = prediction[0].detach().numpy()\n",
    "output = output.squeeze()\n",
    "from utilities.plot_utils import (\n",
    "    plot_prediction_with_thresholds,\n",
    "    plot_random_patch,\n",
    "    plot_output,\n",
    ")\n",
    "\n",
    "plot_output(output)\n",
    "plot_random_patch(output, patch_len=6)\n",
    "plot_prediction_with_thresholds(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
